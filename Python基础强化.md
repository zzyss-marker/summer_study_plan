# ğŸ PythonåŸºç¡€å¼ºåŒ–

## ğŸ¯ å­¦ä¹ ç›®æ ‡
æŒæ¡AIå¼€å‘æ‰€éœ€çš„Pythoné«˜çº§ç‰¹æ€§ï¼Œä¸ºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ‰“ä¸‹åšå®åŸºç¡€ã€‚

## ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹

### [[è£…é¥°å™¨æ¨¡å¼]]
**æ¦‚å¿µ**: åœ¨ä¸ä¿®æ”¹åŸå‡½æ•°çš„æƒ…å†µä¸‹ï¼Œä¸ºå‡½æ•°æ·»åŠ æ–°åŠŸèƒ½çš„è®¾è®¡æ¨¡å¼

#### åŸºç¡€è£…é¥°å™¨
```python
def timer(func):
    def wrapper(*args, **kwargs):
        import time
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {end-start:.4f}ç§’")
        return result
    return wrapper

@timer
def slow_function():
    import time
    time.sleep(1)
    return "å®Œæˆ"
```

#### å¸¦å‚æ•°è£…é¥°å™¨
```python
def retry(max_attempts=3):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise e
                    print(f"ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥: {e}")
        return wrapper
    return decorator

@retry(max_attempts=5)
def unstable_function():
    import random
    if random.random() < 0.7:
        raise Exception("éšæœºå¤±è´¥")
    return "æˆåŠŸ"
```

**åº”ç”¨åœºæ™¯**: 
- æ€§èƒ½ç›‘æ§ â†’ [[æ¨¡å‹è®­ç»ƒç›‘æ§]]
- å¼‚å¸¸å¤„ç† â†’ [[æ•°æ®å¤„ç†å®¹é”™]]
- ç¼“å­˜æœºåˆ¶ â†’ [[è®¡ç®—ç»“æœç¼“å­˜]]

### [[ç”Ÿæˆå™¨ä¸è¿­ä»£å™¨]]
**æ¦‚å¿µ**: å†…å­˜é«˜æ•ˆçš„æ•°æ®å¤„ç†æ–¹å¼ï¼Œç‰¹åˆ«é€‚åˆå¤§æ•°æ®åœºæ™¯

#### ç”Ÿæˆå™¨å‡½æ•°
```python
def data_generator(file_path, batch_size=1000):
    """å¤§æ–‡ä»¶æ‰¹é‡è¯»å–ç”Ÿæˆå™¨"""
    with open(file_path, 'r') as f:
        batch = []
        for line in f:
            batch.append(line.strip())
            if len(batch) == batch_size:
                yield batch
                batch = []
        if batch:  # å¤„ç†æœ€åä¸€æ‰¹æ•°æ®
            yield batch

# ä½¿ç”¨ç¤ºä¾‹
for batch in data_generator('large_dataset.txt'):
    # å¤„ç†æ¯æ‰¹æ•°æ®ï¼Œé¿å…å†…å­˜æº¢å‡º
    process_batch(batch)
```

#### ç”Ÿæˆå™¨è¡¨è¾¾å¼
```python
# å†…å­˜é«˜æ•ˆçš„æ•°æ®å¤„ç†
squared_numbers = (x**2 for x in range(1000000))
filtered_data = (x for x in data if x > threshold)

# é“¾å¼ç”Ÿæˆå™¨
def preprocess_pipeline(data):
    # æ¸…æ´—æ•°æ®
    cleaned = (clean_text(item) for item in data)
    # ç‰¹å¾æå–
    features = (extract_features(item) for item in cleaned)
    # æ ‡å‡†åŒ–
    normalized = (normalize(item) for item in features)
    return normalized
```

**åº”ç”¨åœºæ™¯**:
- [[å¤§æ•°æ®å¤„ç†]] - é¿å…å†…å­˜æº¢å‡º
- [[æ•°æ®æµæ°´çº¿]] - é«˜æ•ˆæ•°æ®é¢„å¤„ç†
- [[å®æ—¶æ•°æ®å¤„ç†]] - æµå¼è®¡ç®—

### [[ä¸Šä¸‹æ–‡ç®¡ç†å™¨]]
**æ¦‚å¿µ**: ç¡®ä¿èµ„æºçš„æ­£ç¡®è·å–å’Œé‡Šæ”¾

#### ç±»å®ç°æ–¹å¼
```python
class DatabaseConnection:
    def __init__(self, db_name):
        self.db_name = db_name
        self.connection = None
    
    def __enter__(self):
        print(f"è¿æ¥åˆ°æ•°æ®åº“: {self.db_name}")
        self.connection = connect_to_db(self.db_name)
        return self.connection
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.connection:
            self.connection.close()
            print("æ•°æ®åº“è¿æ¥å·²å…³é—­")
        if exc_type:
            print(f"å‘ç”Ÿå¼‚å¸¸: {exc_val}")
        return False  # ä¸æŠ‘åˆ¶å¼‚å¸¸

# ä½¿ç”¨ç¤ºä¾‹
with DatabaseConnection('ml_data') as db:
    data = db.query("SELECT * FROM training_data")
    # è‡ªåŠ¨å¤„ç†è¿æ¥å…³é—­
```

#### contextlibå®ç°
```python
from contextlib import contextmanager
import numpy as np

@contextmanager
def numpy_seed(seed):
    """ä¸´æ—¶è®¾ç½®numpyéšæœºç§å­"""
    old_state = np.random.get_state()
    np.random.seed(seed)
    try:
        yield
    finally:
        np.random.set_state(old_state)

# ä½¿ç”¨ç¤ºä¾‹
with numpy_seed(42):
    # å¯é‡ç°çš„éšæœºæ•°ç”Ÿæˆ
    random_data = np.random.randn(1000)
# è‡ªåŠ¨æ¢å¤åŸå§‹éšæœºçŠ¶æ€
```

**åº”ç”¨åœºæ™¯**:
- [[èµ„æºç®¡ç†]] - æ–‡ä»¶ã€æ•°æ®åº“è¿æ¥
- [[çŠ¶æ€ç®¡ç†]] - ä¸´æ—¶é…ç½®ã€ç¯å¢ƒå˜é‡
- [[å¼‚å¸¸å®‰å…¨]] - ç¡®ä¿æ¸…ç†æ“ä½œ

### [[å…ƒç±»ç¼–ç¨‹]]
**æ¦‚å¿µ**: æ§åˆ¶ç±»çš„åˆ›å»ºè¿‡ç¨‹ï¼Œå®ç°é«˜çº§æŠ½è±¡

#### åŸºç¡€å…ƒç±»
```python
class ModelMeta(type):
    """æœºå™¨å­¦ä¹ æ¨¡å‹å…ƒç±»"""
    def __new__(cls, name, bases, attrs):
        # è‡ªåŠ¨æ·»åŠ æ¨¡å‹éªŒè¯æ–¹æ³•
        if 'validate' not in attrs:
            attrs['validate'] = cls.default_validate
        
        # è‡ªåŠ¨æ³¨å†Œæ¨¡å‹
        model_class = super().__new__(cls, name, bases, attrs)
        if name != 'BaseModel':
            ModelRegistry.register(name, model_class)
        
        return model_class
    
    @staticmethod
    def default_validate(self, data):
        """é»˜è®¤éªŒè¯æ–¹æ³•"""
        if data is None or len(data) == 0:
            raise ValueError("æ•°æ®ä¸èƒ½ä¸ºç©º")
        return True

class BaseModel(metaclass=ModelMeta):
    """åŸºç¡€æ¨¡å‹ç±»"""
    pass

class LinearRegression(BaseModel):
    """çº¿æ€§å›å½’æ¨¡å‹ - è‡ªåŠ¨è·å¾—éªŒè¯å’Œæ³¨å†ŒåŠŸèƒ½"""
    def fit(self, X, y):
        self.validate(X)  # è‡ªåŠ¨æ·»åŠ çš„éªŒè¯æ–¹æ³•
        # æ¨¡å‹è®­ç»ƒé€»è¾‘
        pass
```

#### å±æ€§æè¿°ç¬¦
```python
class ValidatedAttribute:
    """éªŒè¯å±æ€§æè¿°ç¬¦"""
    def __init__(self, validator=None, default=None):
        self.validator = validator
        self.default = default
        self.name = None
    
    def __set_name__(self, owner, name):
        self.name = f'_{name}'
    
    def __get__(self, obj, objtype=None):
        if obj is None:
            return self
        return getattr(obj, self.name, self.default)
    
    def __set__(self, obj, value):
        if self.validator:
            value = self.validator(value)
        setattr(obj, self.name, value)

# éªŒè¯å™¨å‡½æ•°
def positive_number(value):
    if not isinstance(value, (int, float)) or value <= 0:
        raise ValueError("å¿…é¡»æ˜¯æ­£æ•°")
    return value

class MLModel:
    """ä½¿ç”¨æè¿°ç¬¦çš„æœºå™¨å­¦ä¹ æ¨¡å‹"""
    learning_rate = ValidatedAttribute(positive_number, 0.01)
    epochs = ValidatedAttribute(lambda x: int(x) if x > 0 else 1, 100)
    
    def __init__(self, learning_rate=None, epochs=None):
        if learning_rate is not None:
            self.learning_rate = learning_rate
        if epochs is not None:
            self.epochs = epochs
```

**åº”ç”¨åœºæ™¯**:
- [[æ¡†æ¶å¼€å‘]] - è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆ
- [[é…ç½®ç®¡ç†]] - åŠ¨æ€ç±»åˆ›å»º
- [[APIè®¾è®¡]] - ç»Ÿä¸€æ¥å£è§„èŒƒ

### [[å‡½æ•°å¼ç¼–ç¨‹]]
**æ¦‚å¿µ**: ä½¿ç”¨å‡½æ•°ä½œä¸ºä¸€ç­‰å…¬æ°‘çš„ç¼–ç¨‹èŒƒå¼

#### é«˜é˜¶å‡½æ•°
```python
from functools import reduce, partial
from operator import add, mul

def compose(*functions):
    """å‡½æ•°ç»„åˆ"""
    return reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)

def curry(func):
    """æŸ¯é‡ŒåŒ–è£…é¥°å™¨"""
    def curried(*args, **kwargs):
        if len(args) + len(kwargs) >= func.__code__.co_argcount:
            return func(*args, **kwargs)
        return partial(func, *args, **kwargs)
    return curried

# æ•°æ®å¤„ç†ç®¡é“
@curry
def filter_data(condition, data):
    return [item for item in data if condition(item)]

@curry
def transform_data(transformer, data):
    return [transformer(item) for item in data]

@curry
def aggregate_data(aggregator, data):
    return reduce(aggregator, data)

# æ„å»ºå¤„ç†ç®¡é“
process_pipeline = compose(
    aggregate_data(add),
    transform_data(lambda x: x ** 2),
    filter_data(lambda x: x > 0)
)

# ä½¿ç”¨ç®¡é“
result = process_pipeline([-2, -1, 0, 1, 2, 3])  # ç»“æœ: 14 (1Â² + 2Â² + 3Â²)
```

#### å‡½æ•°å¼æ•°æ®å¤„ç†
```python
from itertools import chain, groupby, accumulate
from collections import defaultdict

def functional_data_analysis(data):
    """å‡½æ•°å¼æ•°æ®åˆ†æ"""
    # æ•°æ®æ¸…æ´—å’Œè½¬æ¢
    cleaned = map(lambda x: x.strip().lower(), data)
    filtered = filter(lambda x: len(x) > 0, cleaned)
    
    # åˆ†ç»„ç»Ÿè®¡
    grouped = groupby(sorted(filtered))
    counts = {k: len(list(v)) for k, v in grouped}
    
    # ç´¯ç§¯è®¡ç®—
    values = list(counts.values())
    cumulative = list(accumulate(values))
    
    return {
        'counts': counts,
        'total': sum(values),
        'cumulative': cumulative
    }

# å‡½æ•°å¼ç‰¹å¾å·¥ç¨‹
def feature_engineering_pipeline(data):
    """ç‰¹å¾å·¥ç¨‹ç®¡é“"""
    return compose(
        # æ ‡å‡†åŒ–
        lambda x: [(i - np.mean(x)) / np.std(x) for i in x],
        # å¼‚å¸¸å€¼å¤„ç†
        lambda x: [i for i in x if abs(i) < 3],
        # æ•°æ®è½¬æ¢
        lambda x: [float(i) for i in x if str(i).replace('.', '').isdigit()]
    )(data)
```

**åº”ç”¨åœºæ™¯**:
- [[æ•°æ®å¤„ç†ç®¡é“]] - é“¾å¼æ•°æ®è½¬æ¢
- [[ç‰¹å¾å·¥ç¨‹]] - å¯ç»„åˆçš„ç‰¹å¾å˜æ¢
- [[æ¨¡å‹ç»„åˆ]] - é›†æˆå­¦ä¹ 

## ğŸ”— çŸ¥è¯†å…³è”

### ä¸AIå­¦ä¹ çš„è¿æ¥
- [[è£…é¥°å™¨]] â†’ [[æ¨¡å‹è®­ç»ƒç›‘æ§]] â†’ [[å®éªŒç®¡ç†]]
- [[ç”Ÿæˆå™¨]] â†’ [[å¤§æ•°æ®å¤„ç†]] â†’ [[æ‰¹é‡è®­ç»ƒ]]
- [[ä¸Šä¸‹æ–‡ç®¡ç†å™¨]] â†’ [[èµ„æºç®¡ç†]] â†’ [[GPUå†…å­˜ç®¡ç†]]
- [[å…ƒç±»]] â†’ [[æ¡†æ¶è®¾è®¡]] â†’ [[æ¨¡å‹å·¥å‚]]
- [[å‡½æ•°å¼ç¼–ç¨‹]] â†’ [[æ•°æ®ç®¡é“]] â†’ [[ç‰¹å¾å·¥ç¨‹]]

### å®è·µé¡¹ç›®è¿æ¥
- [[Pythonæ€§èƒ½ä¼˜åŒ–]] - è£…é¥°å™¨ + ç”Ÿæˆå™¨
- [[æ•°æ®å¤„ç†æ¡†æ¶]] - ä¸Šä¸‹æ–‡ç®¡ç†å™¨ + å‡½æ•°å¼ç¼–ç¨‹
- [[æœºå™¨å­¦ä¹ åº“]] - å…ƒç±» + æè¿°ç¬¦
- [[å®éªŒç®¡ç†ç³»ç»Ÿ]] - è£…é¥°å™¨ + ä¸Šä¸‹æ–‡ç®¡ç†å™¨

## ğŸ“Š å­¦ä¹ è¿›åº¦

### æŒæ¡ç¨‹åº¦è¯„ä¼°
- [ ] ğŸ”´ åŸºç¡€è¯­æ³• - èƒ½è¯»æ‡‚ä»£ç 
- [ ] ğŸŸ¡ ç†è§£åŸç† - èƒ½è§£é‡Šæ¦‚å¿µ
- [ ] ğŸŸ¢ ç†Ÿç»ƒåº”ç”¨ - èƒ½ç‹¬ç«‹å®ç°
- [ ] ğŸ”µ åˆ›æ–°ä½¿ç”¨ - èƒ½è®¾è®¡æ¨¡å¼

### å®è·µæ£€éªŒ
- [ ] å®ç°è£…é¥°å™¨åº“
- [ ] å¼€å‘æ•°æ®ç”Ÿæˆå™¨
- [ ] è®¾è®¡ä¸Šä¸‹æ–‡ç®¡ç†å™¨
- [ ] åˆ›å»ºå…ƒç±»æ¡†æ¶
- [ ] æ„å»ºå‡½æ•°å¼å·¥å…·

## ğŸ·ï¸ æ ‡ç­¾
`#Python` `#é«˜çº§ç‰¹æ€§` `#ç¼–ç¨‹èŒƒå¼` `#ä»£ç ä¼˜åŒ–` `#AIåŸºç¡€`

## ğŸ“š ç›¸å…³èµ„æº
- [[Pythonè¿›é˜¶ç¼–ç¨‹]] - é«˜çº§ç‰¹æ€§è¯¦è§£
- [[è®¾è®¡æ¨¡å¼Pythonå®ç°]] - æ¨¡å¼åº”ç”¨
- [[å‡½æ•°å¼ç¼–ç¨‹æŒ‡å—]] - èŒƒå¼å­¦ä¹ 
- [[æ€§èƒ½ä¼˜åŒ–æŠ€å·§]] - æ•ˆç‡æå‡

---
**å¯¼èˆª**: [[AIå­¦ä¹ è·¯å¾„å›¾]] | [[æ•°æ®å¤„ç†å·¥å…·]] | [[æœºå™¨å­¦ä¹ ç®—æ³•]]
