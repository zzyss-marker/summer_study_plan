"""
ä»é›¶å¼€å§‹å®ç°ç¥ç»ç½‘ç»œ
ç†è§£ç¥ç»ç½‘ç»œçš„åŸºæœ¬åŸç†å’Œåå‘ä¼ æ’­ç®—æ³•
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_circles
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

class NeuralNetworkFromScratch:
    """ä»é›¶å¼€å§‹å®ç°ç¥ç»ç½‘ç»œ"""
    
    def __init__(self):
        self.examples_completed = []
        print("ğŸ§  ç¥ç»ç½‘ç»œä»é›¶å®ç°ç³»ç»Ÿ")
        print("=" * 50)
    
    def activation_functions_demo(self):
        """æ¿€æ´»å‡½æ•°æ¼”ç¤º"""
        print("ğŸ§  æ¿€æ´»å‡½æ•°æ¼”ç¤º")
        print("=" * 30)
        
        class ActivationFunctions:
            @staticmethod
            def sigmoid(x):
                return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
            
            @staticmethod
            def sigmoid_derivative(x):
                s = ActivationFunctions.sigmoid(x)
                return s * (1 - s)
            
            @staticmethod
            def tanh(x):
                return np.tanh(x)
            
            @staticmethod
            def tanh_derivative(x):
                return 1 - np.tanh(x)**2
            
            @staticmethod
            def relu(x):
                return np.maximum(0, x)
            
            @staticmethod
            def relu_derivative(x):
                return (x > 0).astype(float)
        
        # å¯è§†åŒ–æ¿€æ´»å‡½æ•°
        x = np.linspace(-5, 5, 1000)
        
        plt.figure(figsize=(15, 10))
        
        activations = [
            ('Sigmoid', ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative),
            ('Tanh', ActivationFunctions.tanh, ActivationFunctions.tanh_derivative),
            ('ReLU', ActivationFunctions.relu, ActivationFunctions.relu_derivative)
        ]
        
        for i, (name, func, deriv_func) in enumerate(activations):
            # æ¿€æ´»å‡½æ•°
            plt.subplot(2, 3, i+1)
            y = func(x)
            plt.plot(x, y, linewidth=2)
            plt.title(f'{name}æ¿€æ´»å‡½æ•°')
            plt.xlabel('x')
            plt.ylabel('f(x)')
            plt.grid(True, alpha=0.3)
            
            # å¯¼æ•°
            plt.subplot(2, 3, i+4)
            y_deriv = deriv_func(x)
            plt.plot(x, y_deriv, linewidth=2, color='red')
            plt.title(f'{name}å¯¼æ•°')
            plt.xlabel('x')
            plt.ylabel("f'(x)")
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        print("æ¿€æ´»å‡½æ•°ç‰¹ç‚¹:")
        print("1. Sigmoid: è¾“å‡ºèŒƒå›´(0,1)ï¼Œå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
        print("2. Tanh: è¾“å‡ºèŒƒå›´(-1,1)ï¼Œé›¶ä¸­å¿ƒåŒ–")
        print("3. ReLU: è®¡ç®—ç®€å•ï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œä½†å­˜åœ¨ç¥ç»å…ƒæ­»äº¡")
        
        self.examples_completed.append("æ¿€æ´»å‡½æ•°")
    
    def simple_perceptron(self):
        """ç®€å•æ„ŸçŸ¥æœºå®ç°"""
        print("\nğŸ”— ç®€å•æ„ŸçŸ¥æœºå®ç°")
        print("=" * 30)
        
        class Perceptron:
            def __init__(self, learning_rate=0.01, max_iterations=1000):
                self.learning_rate = learning_rate
                self.max_iterations = max_iterations
                self.weights = None
                self.bias = None
                self.errors = []
            
            def activation(self, x):
                """é˜¶è·ƒæ¿€æ´»å‡½æ•°"""
                return 1 if x >= 0 else 0
            
            def fit(self, X, y):
                n_samples, n_features = X.shape
                
                # åˆå§‹åŒ–æƒé‡å’Œåç½®
                self.weights = np.random.normal(0, 0.01, n_features)
                self.bias = 0
                
                for iteration in range(self.max_iterations):
                    errors = 0
                    
                    for i in range(n_samples):
                        # å‰å‘ä¼ æ’­
                        linear_output = np.dot(X[i], self.weights) + self.bias
                        prediction = self.activation(linear_output)
                        
                        # è®¡ç®—è¯¯å·®
                        error = y[i] - prediction
                        
                        if error != 0:
                            errors += 1
                            # æ›´æ–°æƒé‡å’Œåç½®
                            self.weights += self.learning_rate * error * X[i]
                            self.bias += self.learning_rate * error
                    
                    self.errors.append(errors)
                    
                    # å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œæå‰åœæ­¢
                    if errors == 0:
                        print(f"æ„ŸçŸ¥æœºåœ¨ç¬¬{iteration+1}æ¬¡è¿­ä»£åæ”¶æ•›")
                        break
            
            def predict(self, X):
                linear_output = np.dot(X, self.weights) + self.bias
                return [self.activation(x) for x in linear_output]
        
        # ç”Ÿæˆçº¿æ€§å¯åˆ†æ•°æ®
        X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, 
                                 n_informative=2, n_clusters_per_class=1, 
                                 random_state=42)
        
        # å°†æ ‡ç­¾è½¬æ¢ä¸º0å’Œ1
        y = (y > 0).astype(int)
        
        # è®­ç»ƒæ„ŸçŸ¥æœº
        perceptron = Perceptron(learning_rate=0.1, max_iterations=100)
        perceptron.fit(X, y)
        
        # é¢„æµ‹
        predictions = perceptron.predict(X)
        accuracy = np.mean(predictions == y)
        
        print(f"æ„ŸçŸ¥æœºå‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"æœ€ç»ˆæƒé‡: {perceptron.weights}")
        print(f"æœ€ç»ˆåç½®: {perceptron.bias}")
        
        # å¯è§†åŒ–ç»“æœ
        plt.figure(figsize=(12, 5))
        
        # å†³ç­–è¾¹ç•Œ
        plt.subplot(1, 2, 1)
        
        # ç»˜åˆ¶æ•°æ®ç‚¹
        colors = ['red', 'blue']
        for i in range(2):
            mask = y == i
            plt.scatter(X[mask, 0], X[mask, 1], c=colors[i], 
                       label=f'ç±»åˆ« {i}', alpha=0.7)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        if perceptron.weights[1] != 0:
            x_boundary = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
            y_boundary = -(perceptron.weights[0] * x_boundary + perceptron.bias) / perceptron.weights[1]
            plt.plot(x_boundary, y_boundary, 'k-', linewidth=2, label='å†³ç­–è¾¹ç•Œ')
        
        plt.xlabel('ç‰¹å¾1')
        plt.ylabel('ç‰¹å¾2')
        plt.title('æ„ŸçŸ¥æœºåˆ†ç±»ç»“æœ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # é”™è¯¯æ•°é‡å˜åŒ–
        plt.subplot(1, 2, 2)
        plt.plot(perceptron.errors, 'bo-')
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('é”™è¯¯æ•°é‡')
        plt.title('æ„ŸçŸ¥æœºæ”¶æ•›è¿‡ç¨‹')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        self.examples_completed.append("æ„ŸçŸ¥æœº")
    
    def multilayer_perceptron(self):
        """å¤šå±‚æ„ŸçŸ¥æœºå®ç°"""
        print("\nğŸ§  å¤šå±‚æ„ŸçŸ¥æœºå®ç°")
        print("=" * 30)
        
        class MLP:
            def __init__(self, layers, learning_rate=0.01, max_iterations=1000):
                self.layers = layers
                self.learning_rate = learning_rate
                self.max_iterations = max_iterations
                self.weights = []
                self.biases = []
                self.costs = []
                
                # åˆå§‹åŒ–æƒé‡å’Œåç½®
                for i in range(len(layers) - 1):
                    w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])
                    b = np.zeros((1, layers[i+1]))
                    self.weights.append(w)
                    self.biases.append(b)
            
            def sigmoid(self, x):
                return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
            
            def sigmoid_derivative(self, x):
                s = self.sigmoid(x)
                return s * (1 - s)
            
            def forward_propagation(self, X):
                """å‰å‘ä¼ æ’­"""
                activations = [X]
                z_values = []
                
                for i in range(len(self.weights)):
                    z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
                    z_values.append(z)
                    a = self.sigmoid(z)
                    activations.append(a)
                
                return activations, z_values
            
            def compute_cost(self, y_true, y_pred):
                """è®¡ç®—æŸå¤±å‡½æ•°"""
                m = y_true.shape[0]
                cost = -(1/m) * np.sum(y_true * np.log(y_pred + 1e-8) + 
                                     (1 - y_true) * np.log(1 - y_pred + 1e-8))
                return cost
            
            def fit(self, X, y):
                for iteration in range(self.max_iterations):
                    # å‰å‘ä¼ æ’­
                    activations, z_values = self.forward_propagation(X)
                    
                    # è®¡ç®—æŸå¤±
                    cost = self.compute_cost(y, activations[-1])
                    self.costs.append(cost)
                    
                    # åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                    m = X.shape[0]
                    
                    # è¾“å‡ºå±‚è¯¯å·®
                    output_error = activations[-1] - y
                    
                    # æ›´æ–°è¾“å‡ºå±‚æƒé‡
                    dW_output = (1/m) * np.dot(activations[-2].T, output_error)
                    db_output = (1/m) * np.sum(output_error, axis=0, keepdims=True)
                    
                    self.weights[-1] -= self.learning_rate * dW_output
                    self.biases[-1] -= self.learning_rate * db_output
                    
                    if iteration % 100 == 0:
                        print(f"è¿­ä»£ {iteration}, æŸå¤±: {cost:.6f}")
            
            def predict(self, X):
                activations, _ = self.forward_propagation(X)
                return activations[-1]
            
            def predict_classes(self, X):
                predictions = self.predict(X)
                return (predictions > 0.5).astype(int)
        
        # ç”Ÿæˆéçº¿æ€§å¯åˆ†æ•°æ®
        X, y = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=42)
        y = y.reshape(-1, 1)
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42)
        
        # åˆ›å»ºå’Œè®­ç»ƒMLP
        mlp = MLP(layers=[2, 10, 1], learning_rate=0.1, max_iterations=1000)
        mlp.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = mlp.predict(X_test)
        y_pred_classes = mlp.predict_classes(X_test)
        accuracy = np.mean(y_pred_classes == y_test)
        
        print(f"\nMLPå‡†ç¡®ç‡: {accuracy:.4f}")
        
        # å¯è§†åŒ–ç»“æœ
        plt.figure(figsize=(15, 5))
        
        # åŸå§‹æ•°æ®
        plt.subplot(1, 3, 1)
        colors = ['red', 'blue']
        for i in range(2):
            mask = y.flatten() == i
            plt.scatter(X_scaled[mask, 0], X_scaled[mask, 1], c=colors[i], 
                       label=f'ç±»åˆ« {i}', alpha=0.7)
        plt.xlabel('ç‰¹å¾1')
        plt.ylabel('ç‰¹å¾2')
        plt.title('åŸå§‹æ•°æ®')
        plt.legend()
        
        # å†³ç­–è¾¹ç•Œ
        plt.subplot(1, 3, 2)
        h = 0.02
        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                            np.arange(y_min, y_max, h))
        
        grid_points = np.c_[xx.ravel(), yy.ravel()]
        Z = mlp.predict(grid_points)
        Z = Z.reshape(xx.shape)
        
        plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')
        
        for i in range(2):
            mask = y.flatten() == i
            plt.scatter(X_scaled[mask, 0], X_scaled[mask, 1], c=colors[i], 
                       label=f'ç±»åˆ« {i}', alpha=0.7, edgecolors='black')
        
        plt.xlabel('ç‰¹å¾1')
        plt.ylabel('ç‰¹å¾2')
        plt.title('MLPå†³ç­–è¾¹ç•Œ')
        plt.legend()
        
        # æŸå¤±å‡½æ•°å˜åŒ–
        plt.subplot(1, 3, 3)
        plt.plot(mlp.costs)
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('æŸå¤±')
        plt.title('è®­ç»ƒæŸå¤±å˜åŒ–')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        self.examples_completed.append("å¤šå±‚æ„ŸçŸ¥æœº")
    
    def run_all_examples(self):
        """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
        print("ğŸ§  ç¥ç»ç½‘ç»œåŸºç¡€å®Œæ•´å­¦ä¹ ")
        print("=" * 60)
        
        self.activation_functions_demo()
        self.simple_perceptron()
        self.multilayer_perceptron()
        
        print(f"\nğŸ‰ ç¥ç»ç½‘ç»œåŸºç¡€å­¦ä¹ å®Œæˆï¼")
        print(f"å®Œæˆçš„æ¨¡å—: {', '.join(self.examples_completed)}")
        
        print(f"\nğŸ“š å­¦ä¹ æ€»ç»“:")
        print("1. æ¿€æ´»å‡½æ•° - å¼•å…¥éçº¿æ€§ï¼Œå„æœ‰ä¼˜ç¼ºç‚¹")
        print("2. æ„ŸçŸ¥æœº - æœ€ç®€å•çš„ç¥ç»ç½‘ç»œï¼Œåªèƒ½è§£å†³çº¿æ€§å¯åˆ†é—®é¢˜")
        print("3. å¤šå±‚æ„ŸçŸ¥æœº - é€šè¿‡éšè—å±‚è§£å†³éçº¿æ€§é—®é¢˜")

def main():
    """ä¸»å‡½æ•°"""
    nn = NeuralNetworkFromScratch()
    nn.run_all_examples()
    
    print("\nğŸ’¡ æ·±åº¦å­¦ä¹ å‘å±•:")
    print("1. æ„ŸçŸ¥æœº â†’ å¤šå±‚æ„ŸçŸ¥æœº â†’ æ·±åº¦ç¥ç»ç½‘ç»œ")
    print("2. æ¿€æ´»å‡½æ•°: Sigmoid â†’ ReLU â†’ å„ç§å˜ä½“")
    print("3. ä¼˜åŒ–ç®—æ³•: SGD â†’ Adam â†’ å„ç§è‡ªé€‚åº”æ–¹æ³•")

if __name__ == "__main__":
    main()
