"""
è¡¨æ ¼æ•°æ®æœºå™¨å­¦ä¹ Baselineè„šæœ¬
æ”¯æŒåˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼Œè‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹é€‰æ‹©
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
from sklearn.svm import SVC, SVR
from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score, accuracy_score
import warnings
warnings.filterwarnings('ignore')

class TabularBaseline:
    """è¡¨æ ¼æ•°æ®æœºå™¨å­¦ä¹ Baselineç±»"""
    
    def __init__(self, task_type='auto'):
        """
        åˆå§‹åŒ–
        task_type: 'classification', 'regression', 'auto'
        """
        self.task_type = task_type
        self.models = {}
        self.results = {}
        self.best_model = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        
    def load_data(self, data_path=None, X=None, y=None):
        """åŠ è½½æ•°æ®"""
        if data_path:
            self.data = pd.read_csv(data_path)
            print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå½¢çŠ¶: {self.data.shape}")
        elif X is not None and y is not None:
            self.X = X
            self.y = y
            print(f"æ•°æ®åŠ è½½å®Œæˆï¼ŒXå½¢çŠ¶: {X.shape}, yå½¢çŠ¶: {y.shape}")
        else:
            # ç”Ÿæˆç¤ºä¾‹æ•°æ®
            self._generate_sample_data()
    
    def _generate_sample_data(self):
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®"""
        np.random.seed(42)
        n_samples = 1000
        
        # ç”Ÿæˆç‰¹å¾
        age = np.random.randint(18, 80, n_samples)
        income = np.random.normal(50000, 20000, n_samples)
        education = np.random.choice(['é«˜ä¸­', 'æœ¬ç§‘', 'ç¡•å£«', 'åšå£«'], n_samples)
        experience = np.random.randint(0, 40, n_samples)
        
        # ç”Ÿæˆç›®æ ‡å˜é‡
        if self.task_type == 'classification' or self.task_type == 'auto':
            # åˆ†ç±»ä»»åŠ¡ï¼šé¢„æµ‹æ˜¯å¦é«˜æ”¶å…¥
            target = ((income > 60000) & (age > 30) & (experience > 5)).astype(int)
            self.task_type = 'classification'
        else:
            # å›å½’ä»»åŠ¡ï¼šé¢„æµ‹æ”¶å…¥
            target = income + age * 500 + experience * 1000 + np.random.normal(0, 5000, n_samples)
        
        self.data = pd.DataFrame({
            'age': age,
            'income': income,
            'education': education,
            'experience': experience,
            'target': target
        })
        
        print(f"ç”Ÿæˆç¤ºä¾‹æ•°æ®å®Œæˆï¼Œå½¢çŠ¶: {self.data.shape}")
        print(f"ä»»åŠ¡ç±»å‹: {self.task_type}")
    
    def preprocess_data(self, target_column='target'):
        """æ•°æ®é¢„å¤„ç†"""
        print("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        if hasattr(self, 'data'):
            X = self.data.drop(target_column, axis=1)
            y = self.data[target_column]
        else:
            X, y = self.X, self.y
        
        # è‡ªåŠ¨æ£€æµ‹ä»»åŠ¡ç±»å‹
        if self.task_type == 'auto':
            if len(np.unique(y)) <= 10 and y.dtype in ['int64', 'object']:
                self.task_type = 'classification'
            else:
                self.task_type = 'regression'
        
        # å¤„ç†åˆ†ç±»ç‰¹å¾
        categorical_columns = X.select_dtypes(include=['object']).columns
        numerical_columns = X.select_dtypes(include=[np.number]).columns
        
        X_processed = X.copy()
        
        # ç¼–ç åˆ†ç±»ç‰¹å¾
        for col in categorical_columns:
            le = LabelEncoder()
            X_processed[col] = le.fit_transform(X[col].astype(str))
            self.label_encoders[col] = le
        
        # åˆ†å‰²æ•°æ®
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X_processed, y, test_size=0.2, random_state=42, 
            stratify=y if self.task_type == 'classification' else None
        )
        
        # æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print(f"é¢„å¤„ç†å®Œæˆï¼Œè®­ç»ƒé›†: {self.X_train.shape}, æµ‹è¯•é›†: {self.X_test.shape}")
        print(f"åˆ†ç±»ç‰¹å¾: {list(categorical_columns)}")
        print(f"æ•°å€¼ç‰¹å¾: {list(numerical_columns)}")
    
    def setup_models(self):
        """è®¾ç½®æ¨¡å‹"""
        if self.task_type == 'classification':
            self.models = {
                'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
                'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
                'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),
                'SVM': SVC(random_state=42, probability=True)
            }
        else:
            self.models = {
                'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
                'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
                'LinearRegression': LinearRegression(),
                'Ridge': Ridge(random_state=42),
                'Lasso': Lasso(random_state=42),
                'SVR': SVR()
            }
    
    def train_models(self):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print(f"å¼€å§‹è®­ç»ƒ{self.task_type}æ¨¡å‹...")
        
        for name, model in self.models.items():
            print(f"è®­ç»ƒ {name}...")
            
            # é€‰æ‹©æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®
            if name in ['LogisticRegression', 'SVM', 'SVR', 'Ridge', 'Lasso']:
                X_train, X_test = self.X_train_scaled, self.X_test_scaled
            else:
                X_train, X_test = self.X_train, self.X_test
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train, self.y_train)
            
            # é¢„æµ‹
            y_pred = model.predict(X_test)
            
            # è¯„ä¼°
            if self.task_type == 'classification':
                accuracy = accuracy_score(self.y_test, y_pred)
                cv_scores = cross_val_score(model, X_train, self.y_train, cv=5, scoring='accuracy')
                
                self.results[name] = {
                    'model': model,
                    'predictions': y_pred,
                    'accuracy': accuracy,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std()
                }
                
                print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
                print(f"  äº¤å‰éªŒè¯: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
                
            else:
                mse = mean_squared_error(self.y_test, y_pred)
                rmse = np.sqrt(mse)
                r2 = r2_score(self.y_test, y_pred)
                cv_scores = cross_val_score(model, X_train, self.y_train, cv=5, scoring='r2')
                
                self.results[name] = {
                    'model': model,
                    'predictions': y_pred,
                    'mse': mse,
                    'rmse': rmse,
                    'r2': r2,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std()
                }
                
                print(f"  RMSE: {rmse:.4f}")
                print(f"  RÂ²: {r2:.4f}")
                print(f"  äº¤å‰éªŒè¯RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    
    def evaluate_models(self):
        """è¯„ä¼°å’Œæ¯”è¾ƒæ¨¡å‹"""
        print(f"\næ¨¡å‹è¯„ä¼°ç»“æœ:")
        print("=" * 50)
        
        if self.task_type == 'classification':
            # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
            best_model_name = max(self.results.keys(), 
                                key=lambda x: self.results[x]['accuracy'])
            self.best_model = self.results[best_model_name]['model']
            
            # æ‰“å°ç»“æœ
            for name, result in self.results.items():
                print(f"{name}:")
                print(f"  å‡†ç¡®ç‡: {result['accuracy']:.4f}")
                print(f"  äº¤å‰éªŒè¯: {result['cv_mean']:.4f} Â± {result['cv_std']:.4f}")
                if name == best_model_name:
                    print("  â­ æœ€ä½³æ¨¡å‹")
                print()
            
            # è¯¦ç»†è¯„ä¼°æœ€ä½³æ¨¡å‹
            print(f"æœ€ä½³æ¨¡å‹ ({best_model_name}) è¯¦ç»†è¯„ä¼°:")
            best_pred = self.results[best_model_name]['predictions']
            print(classification_report(self.y_test, best_pred))
            
        else:
            # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
            best_model_name = max(self.results.keys(), 
                                key=lambda x: self.results[x]['r2'])
            self.best_model = self.results[best_model_name]['model']
            
            # æ‰“å°ç»“æœ
            for name, result in self.results.items():
                print(f"{name}:")
                print(f"  RMSE: {result['rmse']:.4f}")
                print(f"  RÂ²: {result['r2']:.4f}")
                print(f"  äº¤å‰éªŒè¯RÂ²: {result['cv_mean']:.4f} Â± {result['cv_std']:.4f}")
                if name == best_model_name:
                    print("  â­ æœ€ä½³æ¨¡å‹")
                print()
        
        return best_model_name
    
    def visualize_results(self):
        """å¯è§†åŒ–ç»“æœ"""
        plt.figure(figsize=(15, 10))
        
        if self.task_type == 'classification':
            # å‡†ç¡®ç‡æ¯”è¾ƒ
            plt.subplot(2, 3, 1)
            names = list(self.results.keys())
            accuracies = [self.results[name]['accuracy'] for name in names]
            
            bars = plt.bar(names, accuracies, alpha=0.7)
            plt.title('æ¨¡å‹å‡†ç¡®ç‡æ¯”è¾ƒ')
            plt.ylabel('å‡†ç¡®ç‡')
            plt.xticks(rotation=45)
            
            # æ ‡æ³¨æœ€ä½³æ¨¡å‹
            best_idx = accuracies.index(max(accuracies))
            bars[best_idx].set_color('red')
            
            # æ··æ·†çŸ©é˜µ
            plt.subplot(2, 3, 2)
            best_name = max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])
            best_pred = self.results[best_name]['predictions']
            
            cm = confusion_matrix(self.y_test, best_pred)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title(f'æ··æ·†çŸ©é˜µ ({best_name})')
            plt.ylabel('çœŸå®å€¼')
            plt.xlabel('é¢„æµ‹å€¼')
            
        else:
            # RÂ²æ¯”è¾ƒ
            plt.subplot(2, 3, 1)
            names = list(self.results.keys())
            r2_scores = [self.results[name]['r2'] for name in names]
            
            bars = plt.bar(names, r2_scores, alpha=0.7)
            plt.title('æ¨¡å‹RÂ²æ¯”è¾ƒ')
            plt.ylabel('RÂ²å¾—åˆ†')
            plt.xticks(rotation=45)
            
            # æ ‡æ³¨æœ€ä½³æ¨¡å‹
            best_idx = r2_scores.index(max(r2_scores))
            bars[best_idx].set_color('red')
            
            # é¢„æµ‹vsçœŸå®å€¼
            plt.subplot(2, 3, 2)
            best_name = max(self.results.keys(), key=lambda x: self.results[x]['r2'])
            best_pred = self.results[best_name]['predictions']
            
            plt.scatter(self.y_test, best_pred, alpha=0.6)
            plt.plot([self.y_test.min(), self.y_test.max()], 
                    [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
            plt.xlabel('çœŸå®å€¼')
            plt.ylabel('é¢„æµ‹å€¼')
            plt.title(f'é¢„æµ‹vsçœŸå® ({best_name})')
        
        # äº¤å‰éªŒè¯å¾—åˆ†
        plt.subplot(2, 3, 3)
        cv_means = [self.results[name]['cv_mean'] for name in names]
        cv_stds = [self.results[name]['cv_std'] for name in names]
        
        plt.errorbar(range(len(names)), cv_means, yerr=cv_stds, 
                    fmt='o', capsize=5, capthick=2)
        plt.xticks(range(len(names)), names, rotation=45)
        plt.title('äº¤å‰éªŒè¯å¾—åˆ†')
        plt.ylabel('CVå¾—åˆ†')
        
        plt.tight_layout()
        plt.show()
    
    def run_baseline(self, data_path=None, X=None, y=None, target_column='target'):
        """è¿è¡Œå®Œæ•´çš„baselineæµç¨‹"""
        print("ğŸš€ å¼€å§‹è¡¨æ ¼æ•°æ®æœºå™¨å­¦ä¹ Baseline")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data(data_path, X, y)
        
        # 2. æ•°æ®é¢„å¤„ç†
        self.preprocess_data(target_column)
        
        # 3. è®¾ç½®æ¨¡å‹
        self.setup_models()
        
        # 4. è®­ç»ƒæ¨¡å‹
        self.train_models()
        
        # 5. è¯„ä¼°æ¨¡å‹
        best_model_name = self.evaluate_models()
        
        # 6. å¯è§†åŒ–ç»“æœ
        self.visualize_results()
        
        print(f"\nğŸ‰ Baselineå®Œæˆï¼æœ€ä½³æ¨¡å‹: {best_model_name}")
        return self.best_model, self.results

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    # åˆ†ç±»ä»»åŠ¡ç¤ºä¾‹
    print("=" * 60)
    print("åˆ†ç±»ä»»åŠ¡ç¤ºä¾‹")
    print("=" * 60)
    
    classifier = TabularBaseline(task_type='classification')
    best_clf, clf_results = classifier.run_baseline()
    
    print("\n" + "=" * 60)
    print("å›å½’ä»»åŠ¡ç¤ºä¾‹")
    print("=" * 60)
    
    # å›å½’ä»»åŠ¡ç¤ºä¾‹
    regressor = TabularBaseline(task_type='regression')
    best_reg, reg_results = regressor.run_baseline()
    
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. è‡ªå®šä¹‰æ•°æ®: baseline.run_baseline(data_path='your_data.csv')")
    print("2. æŒ‡å®šç›®æ ‡åˆ—: baseline.run_baseline(target_column='your_target')")
    print("3. ä½¿ç”¨numpyæ•°ç»„: baseline.run_baseline(X=X_array, y=y_array)")

if __name__ == "__main__":
    main()
