"""
è‡ªç„¶è¯­è¨€å¤„ç†æ·±åº¦å­¦ä¹ Baselineè„šæœ¬
æ”¯æŒæ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ï¼Œä½¿ç”¨PyTorchå’ŒTransformerså®ç°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import string

# å°è¯•å¯¼å…¥transformersï¼Œå¦‚æœæ²¡æœ‰å®‰è£…åˆ™ä½¿ç”¨åŸºç¡€æ¨¡å‹
try:
    from transformers import AutoTokenizer, AutoModel, AdamW
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("è­¦å‘Š: transformersåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€LSTMæ¨¡å‹")

class TextDataset(Dataset):
    """æ–‡æœ¬æ•°æ®é›†ç±»"""
    
    def __init__(self, texts, labels, tokenizer=None, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        if tokenizer is None:
            # ä½¿ç”¨ç®€å•çš„è¯æ±‡è¡¨
            self.vocab = self._build_vocab(texts)
            self.vocab_size = len(self.vocab)
    
    def _build_vocab(self, texts):
        """æ„å»ºè¯æ±‡è¡¨"""
        all_words = []
        for text in texts:
            words = self._simple_tokenize(text)
            all_words.extend(words)
        
        word_counts = Counter(all_words)
        vocab = {'<PAD>': 0, '<UNK>': 1}
        
        for word, count in word_counts.most_common(10000):  # é™åˆ¶è¯æ±‡è¡¨å¤§å°
            if count >= 2:  # è¿‡æ»¤ä½é¢‘è¯
                vocab[word] = len(vocab)
        
        return vocab
    
    def _simple_tokenize(self, text):
        """ç®€å•åˆ†è¯"""
        text = text.lower()
        text = re.sub(f'[{string.punctuation}]', ' ', text)
        return text.split()
    
    def _encode_text(self, text):
        """ç¼–ç æ–‡æœ¬"""
        if self.tokenizer:
            # ä½¿ç”¨transformers tokenizer
            encoding = self.tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=self.max_length,
                return_tensors='pt'
            )
            return encoding
        else:
            # ä½¿ç”¨ç®€å•ç¼–ç 
            words = self._simple_tokenize(text)
            indices = [self.vocab.get(word, 1) for word in words]  # 1æ˜¯<UNK>
            
            # æˆªæ–­æˆ–å¡«å……
            if len(indices) > self.max_length:
                indices = indices[:self.max_length]
            else:
                indices.extend([0] * (self.max_length - len(indices)))  # 0æ˜¯<PAD>
            
            return torch.tensor(indices, dtype=torch.long)
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoded = self._encode_text(text)
        
        if self.tokenizer:
            return {
                'input_ids': encoded['input_ids'].squeeze(),
                'attention_mask': encoded['attention_mask'].squeeze(),
                'label': torch.tensor(label, dtype=torch.long)
            }
        else:
            return {
                'input_ids': encoded,
                'label': torch.tensor(label, dtype=torch.long)
            }

class SimpleLSTM(nn.Module):
    """ç®€å•LSTMæ¨¡å‹"""
    
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_classes=2, num_layers=2):
        super(SimpleLSTM, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, 
                           batch_first=True, dropout=0.3, bidirectional=True)
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional
    
    def forward(self, input_ids, attention_mask=None):
        embedded = self.embedding(input_ids)
        
        if attention_mask is not None:
            # å¤„ç†padding
            lengths = attention_mask.sum(dim=1).cpu()
            packed = nn.utils.rnn.pack_padded_sequence(
                embedded, lengths, batch_first=True, enforce_sorted=False
            )
            lstm_out, _ = self.lstm(packed)
            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)
        else:
            lstm_out, _ = self.lstm(embedded)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_hidden = lstm_out[:, -1, :]
        dropped = self.dropout(last_hidden)
        output = self.fc(dropped)
        
        return output

class TransformerModel(nn.Module):
    """åŸºäºé¢„è®­ç»ƒTransformerçš„æ¨¡å‹"""
    
    def __init__(self, model_name='bert-base-uncased', num_classes=2):
        super(TransformerModel, self).__init__()
        
        self.transformer = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_classes)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        dropped = self.dropout(pooled_output)
        logits = self.classifier(dropped)
        
        return logits

class NLPBaseline:
    """NLP Baselineç±»"""
    
    def __init__(self, task_type='classification', num_classes=2, max_length=128):
        """
        åˆå§‹åŒ–
        task_type: 'classification', 'sentiment'
        num_classes: ç±»åˆ«æ•°é‡
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.task_type = task_type
        self.num_classes = num_classes
        self.max_length = max_length
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"Transformerså¯ç”¨: {TRANSFORMERS_AVAILABLE}")
    
    def create_sample_dataset(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†"""
        print("åˆ›å»ºç¤ºä¾‹æ•°æ®é›†...")
        
        # åˆ›å»ºæƒ…æ„Ÿåˆ†æç¤ºä¾‹æ•°æ®
        positive_texts = [
            "I love this movie, it's amazing!",
            "Great product, highly recommended!",
            "Excellent service and quality.",
            "This is the best thing ever!",
            "Wonderful experience, very satisfied.",
            "Outstanding performance and design.",
            "Perfect solution for my needs.",
            "Incredible value for money.",
            "Fantastic quality and service.",
            "Amazing results, exceeded expectations."
        ] * 50  # é‡å¤ä»¥å¢åŠ æ•°æ®é‡
        
        negative_texts = [
            "This movie is terrible and boring.",
            "Poor quality, waste of money.",
            "Disappointing service and product.",
            "Worst experience ever, avoid it.",
            "Completely unsatisfied with this.",
            "Terrible quality and poor design.",
            "Not worth the money at all.",
            "Very disappointed with results.",
            "Poor customer service experience.",
            "Low quality and overpriced."
        ] * 50
        
        # ç»„åˆæ•°æ®
        texts = positive_texts + negative_texts
        labels = [1] * len(positive_texts) + [0] * len(negative_texts)
        
        # æ‰“ä¹±æ•°æ®
        combined = list(zip(texts, labels))
        np.random.shuffle(combined)
        texts, labels = zip(*combined)
        
        # åˆ†å‰²æ•°æ®
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            texts, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        print(f"æ•°æ®é›†åˆ›å»ºå®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(self.X_train)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(self.X_test)} æ ·æœ¬")
        print(f"  ç±»åˆ«æ•°: {self.num_classes}")
    
    def create_model(self, model_type='lstm', model_name='bert-base-uncased'):
        """åˆ›å»ºæ¨¡å‹"""
        print(f"åˆ›å»ºæ¨¡å‹: {model_type}")
        
        if model_type == 'transformer' and TRANSFORMERS_AVAILABLE:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = TransformerModel(model_name, self.num_classes)
        else:
            # ä½¿ç”¨LSTMæ¨¡å‹
            if model_type == 'transformer':
                print("Transformersä¸å¯ç”¨ï¼Œä½¿ç”¨LSTMæ¨¡å‹")
            
            # åˆ›å»ºæ•°æ®é›†ä»¥æ„å»ºè¯æ±‡è¡¨
            train_dataset = TextDataset(self.X_train, self.y_train, max_length=self.max_length)
            vocab_size = train_dataset.vocab_size
            
            self.model = SimpleLSTM(
                vocab_size=vocab_size,
                embedding_dim=128,
                hidden_dim=128,
                num_classes=self.num_classes
            )
            
            self.vocab = train_dataset.vocab
        
        self.model = self.model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    def create_data_loaders(self, batch_size=16):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        train_dataset = TextDataset(
            self.X_train, self.y_train, self.tokenizer, self.max_length
        )
        test_dataset = TextDataset(
            self.X_test, self.y_test, self.tokenizer, self.max_length
        )
        
        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œbatch_size: {batch_size}")
    
    def train_model(self, epochs=5, learning_rate=2e-5):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"å¼€å§‹è®­ç»ƒï¼Œepochs: {epochs}")
        
        criterion = nn.CrossEntropyLoss()
        
        if TRANSFORMERS_AVAILABLE and isinstance(self.model, TransformerModel):
            optimizer = AdamW(self.model.parameters(), lr=learning_rate)
        else:
            optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            running_loss = 0.0
            correct = 0
            total = 0
            
            for batch_idx, batch in enumerate(self.train_loader):
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['label'].to(self.device)
                
                optimizer.zero_grad()
                
                if 'attention_mask' in batch:
                    attention_mask = batch['attention_mask'].to(self.device)
                    outputs = self.model(input_ids, attention_mask)
                else:
                    outputs = self.model(input_ids)
                
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                if batch_idx % 10 == 0:
                    print(f'Epoch: {epoch+1}/{epochs}, Batch: {batch_idx}, '
                          f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')
            
            train_loss = running_loss / len(self.train_loader)
            train_acc = 100. * correct / total
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self.evaluate_model()
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            
            print(f'Epoch {epoch+1}/{epochs}:')
            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
            print('-' * 50)
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        test_loss = 0
        correct = 0
        total = 0
        all_preds = []
        all_targets = []
        
        criterion = nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for batch in self.test_loader:
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['label'].to(self.device)
                
                if 'attention_mask' in batch:
                    attention_mask = batch['attention_mask'].to(self.device)
                    outputs = self.model(input_ids, attention_mask)
                else:
                    outputs = self.model(input_ids)
                
                test_loss += criterion(outputs, labels).item()
                
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_targets.extend(labels.cpu().numpy())
        
        test_loss /= len(self.test_loader)
        accuracy = 100. * correct / total
        
        return test_loss, accuracy
    
    def visualize_results(self):
        """å¯è§†åŒ–è®­ç»ƒç»“æœ"""
        plt.figure(figsize=(15, 10))
        
        # è®­ç»ƒå†å²
        plt.subplot(2, 3, 1)
        plt.plot(self.history['train_loss'], label='Train Loss')
        plt.plot(self.history['val_loss'], label='Val Loss')
        plt.title('è®­ç»ƒæŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(2, 3, 2)
        plt.plot(self.history['train_acc'], label='Train Acc')
        plt.plot(self.history['val_acc'], label='Val Acc')
        plt.title('è®­ç»ƒå‡†ç¡®ç‡')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.legend()
        plt.grid(True)
        
        # æ··æ·†çŸ©é˜µ
        plt.subplot(2, 3, 3)
        self.model.eval()
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for batch in self.test_loader:
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['label'].to(self.device)
                
                if 'attention_mask' in batch:
                    attention_mask = batch['attention_mask'].to(self.device)
                    outputs = self.model(input_ids, attention_mask)
                else:
                    outputs = self.model(input_ids)
                
                _, predicted = outputs.max(1)
                all_preds.extend(predicted.cpu().numpy())
                all_targets.extend(labels.cpu().numpy())
        
        cm = confusion_matrix(all_targets, all_preds)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°åˆ†ç±»æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        class_names = ['Negative', 'Positive'] if self.num_classes == 2 else [f'Class_{i}' for i in range(self.num_classes)]
        print(classification_report(all_targets, all_preds, target_names=class_names))
    
    def predict_text(self, text):
        """é¢„æµ‹å•ä¸ªæ–‡æœ¬"""
        self.model.eval()
        
        # åˆ›å»ºä¸´æ—¶æ•°æ®é›†
        temp_dataset = TextDataset([text], [0], self.tokenizer, self.max_length)
        temp_loader = DataLoader(temp_dataset, batch_size=1, shuffle=False)
        
        with torch.no_grad():
            for batch in temp_loader:
                input_ids = batch['input_ids'].to(self.device)
                
                if 'attention_mask' in batch:
                    attention_mask = batch['attention_mask'].to(self.device)
                    outputs = self.model(input_ids, attention_mask)
                else:
                    outputs = self.model(input_ids)
                
                probabilities = torch.softmax(outputs, dim=1)
                predicted_class = outputs.argmax(dim=1).item()
                confidence = probabilities[0][predicted_class].item()
                
                return predicted_class, confidence
    
    def run_baseline(self, model_type='lstm', epochs=5, learning_rate=2e-5, batch_size=16):
        """è¿è¡Œå®Œæ•´çš„baselineæµç¨‹"""
        print("ğŸš€ å¼€å§‹è‡ªç„¶è¯­è¨€å¤„ç†æ·±åº¦å­¦ä¹ Baseline")
        print("=" * 60)
        
        # 1. åˆ›å»ºæ•°æ®é›†
        self.create_sample_dataset()
        
        # 2. åˆ›å»ºæ¨¡å‹
        self.create_model(model_type)
        
        # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.create_data_loaders(batch_size)
        
        # 4. è®­ç»ƒæ¨¡å‹
        self.train_model(epochs, learning_rate)
        
        # 5. è¯„ä¼°æ¨¡å‹
        final_loss, final_acc = self.evaluate_model()
        
        # 6. å¯è§†åŒ–ç»“æœ
        self.visualize_results()
        
        # 7. æµ‹è¯•é¢„æµ‹
        test_texts = [
            "This is absolutely amazing!",
            "I hate this product, it's terrible."
        ]
        
        print(f"\næµ‹è¯•é¢„æµ‹:")
        for text in test_texts:
            pred_class, confidence = self.predict_text(text)
            sentiment = "Positive" if pred_class == 1 else "Negative"
            print(f"æ–‡æœ¬: '{text}'")
            print(f"é¢„æµ‹: {sentiment} (ç½®ä¿¡åº¦: {confidence:.4f})")
            print()
        
        print(f"ğŸ‰ Baselineå®Œæˆï¼")
        print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.2f}%")
        
        return self.model, final_acc

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    print("=" * 60)
    print("è‡ªç„¶è¯­è¨€å¤„ç†Baselineç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºbaselineå®ä¾‹
    nlp_baseline = NLPBaseline(num_classes=2, max_length=64)
    
    # è¿è¡Œbaseline
    model, accuracy = nlp_baseline.run_baseline(
        model_type='lstm',  # ä½¿ç”¨LSTMæ¨¡å‹
        epochs=3,  # å°‘é‡epochsç”¨äºæ¼”ç¤º
        learning_rate=0.001,
        batch_size=16
    )
    
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. ä½¿ç”¨Transformer: run_baseline(model_type='transformer')")
    print("2. è°ƒæ•´è®­ç»ƒ: run_baseline(epochs=10, learning_rate=2e-5)")
    print("3. è‡ªå®šä¹‰æ•°æ®: æ›¿æ¢create_sample_datasetæ–¹æ³•")
    print("4. æ”¯æŒçš„æ¨¡å‹: lstm, transformer (éœ€è¦transformersåº“)")

if __name__ == "__main__":
    main()
