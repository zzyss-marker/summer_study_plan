"""
è®¡ç®—æœºè§†è§‰æ·±åº¦å­¦ä¹ Baselineè„šæœ¬
æ”¯æŒå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ï¼Œä½¿ç”¨PyTorchå®ç°
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, random_split
import torchvision
import torchvision.transforms as transforms
from torchvision import models
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

class CVBaseline:
    """è®¡ç®—æœºè§†è§‰Baselineç±»"""
    
    def __init__(self, task_type='classification', num_classes=10, image_size=224):
        """
        åˆå§‹åŒ–
        task_type: 'classification', 'detection'
        num_classes: ç±»åˆ«æ•°é‡
        image_size: å›¾åƒå¤§å°
        """
        self.task_type = task_type
        self.num_classes = num_classes
        self.image_size = image_size
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.train_loader = None
        self.test_loader = None
        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def create_sample_dataset(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†"""
        print("åˆ›å»ºç¤ºä¾‹æ•°æ®é›†...")
        
        # ä½¿ç”¨CIFAR-10ä½œä¸ºç¤ºä¾‹
        transform_train = transforms.Compose([
            transforms.Resize((self.image_size, self.image_size)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
        ])
        
        transform_test = transforms.Compose([
            transforms.Resize((self.image_size, self.image_size)),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
        ])
        
        # ä¸‹è½½CIFAR-10æ•°æ®é›†
        train_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True, transform=transform_train
        )
        
        test_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=transform_test
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
        self.test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)
        
        self.num_classes = 10
        self.class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
                           'dog', 'frog', 'horse', 'ship', 'truck']
        
        print(f"æ•°æ®é›†åŠ è½½å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        print(f"  ç±»åˆ«æ•°: {self.num_classes}")
    
    def create_model(self, model_type='resnet18', pretrained=True):
        """åˆ›å»ºæ¨¡å‹"""
        print(f"åˆ›å»ºæ¨¡å‹: {model_type}")
        
        if model_type == 'resnet18':
            self.model = models.resnet18(pretrained=pretrained)
            self.model.fc = nn.Linear(self.model.fc.in_features, self.num_classes)
            
        elif model_type == 'resnet50':
            self.model = models.resnet50(pretrained=pretrained)
            self.model.fc = nn.Linear(self.model.fc.in_features, self.num_classes)
            
        elif model_type == 'vgg16':
            self.model = models.vgg16(pretrained=pretrained)
            self.model.classifier[6] = nn.Linear(self.model.classifier[6].in_features, self.num_classes)
            
        elif model_type == 'efficientnet':
            self.model = models.efficientnet_b0(pretrained=pretrained)
            self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, self.num_classes)
            
        elif model_type == 'simple_cnn':
            self.model = self._create_simple_cnn()
        
        self.model = self.model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    def _create_simple_cnn(self):
        """åˆ›å»ºç®€å•çš„CNNæ¨¡å‹"""
        class SimpleCNN(nn.Module):
            def __init__(self, num_classes):
                super(SimpleCNN, self).__init__()
                self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
                self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
                
                self.pool = nn.MaxPool2d(2, 2)
                self.dropout = nn.Dropout(0.5)
                
                # è®¡ç®—å…¨è¿æ¥å±‚è¾“å…¥å¤§å°
                self.fc1 = nn.Linear(128 * (self.image_size // 8) * (self.image_size // 8), 512)
                self.fc2 = nn.Linear(512, num_classes)
            
            def forward(self, x):
                x = self.pool(F.relu(self.conv1(x)))
                x = self.pool(F.relu(self.conv2(x)))
                x = self.pool(F.relu(self.conv3(x)))
                
                x = x.view(x.size(0), -1)
                x = self.dropout(F.relu(self.fc1(x)))
                x = self.fc2(x)
                return x
        
        return SimpleCNN(self.num_classes)
    
    def train_model(self, epochs=10, learning_rate=0.001):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"å¼€å§‹è®­ç»ƒï¼Œepochs: {epochs}")
        
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            running_loss = 0.0
            correct = 0
            total = 0
            
            for batch_idx, (data, target) in enumerate(self.train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
                
                if batch_idx % 100 == 0:
                    print(f'Epoch: {epoch+1}/{epochs}, Batch: {batch_idx}, '
                          f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')
            
            train_loss = running_loss / len(self.train_loader)
            train_acc = 100. * correct / total
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self.evaluate_model()
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            
            scheduler.step()
            
            print(f'Epoch {epoch+1}/{epochs}:')
            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
            print('-' * 50)
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        test_loss = 0
        correct = 0
        total = 0
        all_preds = []
        all_targets = []
        
        criterion = nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                test_loss += criterion(output, target).item()
                
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        test_loss /= len(self.test_loader)
        accuracy = 100. * correct / total
        
        return test_loss, accuracy
    
    def visualize_results(self):
        """å¯è§†åŒ–è®­ç»ƒç»“æœ"""
        plt.figure(figsize=(15, 10))
        
        # è®­ç»ƒå†å²
        plt.subplot(2, 3, 1)
        plt.plot(self.history['train_loss'], label='Train Loss')
        plt.plot(self.history['val_loss'], label='Val Loss')
        plt.title('è®­ç»ƒæŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(2, 3, 2)
        plt.plot(self.history['train_acc'], label='Train Acc')
        plt.plot(self.history['val_acc'], label='Val Acc')
        plt.title('è®­ç»ƒå‡†ç¡®ç‡')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.legend()
        plt.grid(True)
        
        # æ··æ·†çŸ©é˜µ
        plt.subplot(2, 3, 3)
        self.model.eval()
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                _, predicted = output.max(1)
                all_preds.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        cm = confusion_matrix(all_targets, all_preds)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        
        # æ˜¾ç¤ºä¸€äº›æµ‹è¯•æ ·æœ¬
        plt.subplot(2, 3, 4)
        self._show_sample_predictions()
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°åˆ†ç±»æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(all_targets, all_preds, 
                                  target_names=self.class_names))
    
    def _show_sample_predictions(self):
        """æ˜¾ç¤ºæ ·æœ¬é¢„æµ‹ç»“æœ"""
        self.model.eval()
        
        # è·å–ä¸€ä¸ªbatchçš„æ•°æ®
        data_iter = iter(self.test_loader)
        images, labels = next(data_iter)
        images, labels = images.to(self.device), labels.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(images)
            _, predicted = torch.max(outputs, 1)
        
        # æ˜¾ç¤ºå‰8ä¸ªæ ·æœ¬
        fig, axes = plt.subplots(2, 4, figsize=(12, 6))
        axes = axes.ravel()
        
        for i in range(8):
            img = images[i].cpu()
            # åæ ‡å‡†åŒ–
            img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            img = img + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            img = torch.clamp(img, 0, 1)
            
            axes[i].imshow(img.permute(1, 2, 0))
            axes[i].set_title(f'çœŸå®: {self.class_names[labels[i]]}\n'
                            f'é¢„æµ‹: {self.class_names[predicted[i]]}')
            axes[i].axis('off')
        
        plt.tight_layout()
    
    def save_model(self, path='cv_baseline_model.pth'):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'num_classes': self.num_classes,
            'image_size': self.image_size,
            'class_names': self.class_names
        }, path)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    def load_model(self, path, model_type='resnet18'):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        
        self.num_classes = checkpoint['num_classes']
        self.image_size = checkpoint['image_size']
        self.class_names = checkpoint['class_names']
        
        self.create_model(model_type, pretrained=False)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"æ¨¡å‹å·²ä» {path} åŠ è½½")
    
    def run_baseline(self, model_type='resnet18', epochs=10, learning_rate=0.001):
        """è¿è¡Œå®Œæ•´çš„baselineæµç¨‹"""
        print("ğŸš€ å¼€å§‹è®¡ç®—æœºè§†è§‰æ·±åº¦å­¦ä¹ Baseline")
        print("=" * 60)
        
        # 1. åˆ›å»ºæ•°æ®é›†
        self.create_sample_dataset()
        
        # 2. åˆ›å»ºæ¨¡å‹
        self.create_model(model_type)
        
        # 3. è®­ç»ƒæ¨¡å‹
        self.train_model(epochs, learning_rate)
        
        # 4. è¯„ä¼°æ¨¡å‹
        final_loss, final_acc = self.evaluate_model()
        
        # 5. å¯è§†åŒ–ç»“æœ
        self.visualize_results()
        
        print(f"\nğŸ‰ Baselineå®Œæˆï¼")
        print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.2f}%")
        
        return self.model, final_acc

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    print("=" * 60)
    print("è®¡ç®—æœºè§†è§‰Baselineç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºbaselineå®ä¾‹
    cv_baseline = CVBaseline(num_classes=10, image_size=32)  # CIFAR-10ä½¿ç”¨32x32
    
    # è¿è¡Œbaseline
    model, accuracy = cv_baseline.run_baseline(
        model_type='simple_cnn',  # ä½¿ç”¨ç®€å•CNNä»¥ä¾¿å¿«é€Ÿè®­ç»ƒ
        epochs=5,  # å°‘é‡epochsç”¨äºæ¼”ç¤º
        learning_rate=0.001
    )
    
    # ä¿å­˜æ¨¡å‹
    cv_baseline.save_model('cv_baseline_demo.pth')
    
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. æ›´æ¢æ¨¡å‹: run_baseline(model_type='resnet18')")
    print("2. è°ƒæ•´è®­ç»ƒ: run_baseline(epochs=20, learning_rate=0.0001)")
    print("3. è‡ªå®šä¹‰æ•°æ®é›†: ç»§æ‰¿Datasetç±»å¹¶æ›¿æ¢create_sample_datasetæ–¹æ³•")
    print("4. æ”¯æŒçš„æ¨¡å‹: resnet18, resnet50, vgg16, efficientnet, simple_cnn")

if __name__ == "__main__":
    main()
