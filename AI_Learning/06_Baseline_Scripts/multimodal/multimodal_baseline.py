"""
å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ Baselineè„šæœ¬
æ”¯æŒå›¾åƒ+æ–‡æœ¬çš„å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ç­‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
import torchvision.models as models
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import json
import os
from sklearn.metrics import accuracy_score
import random

class MultimodalDataset(Dataset):
    """å¤šæ¨¡æ€æ•°æ®é›†ç±»"""
    
    def __init__(self, image_paths, texts, labels, transform=None, max_text_length=50):
        self.image_paths = image_paths
        self.texts = texts
        self.labels = labels
        self.transform = transform
        self.max_text_length = max_text_length
        
        # æ„å»ºæ–‡æœ¬è¯æ±‡è¡¨
        self.vocab = self._build_vocab(texts)
        self.vocab_size = len(self.vocab)
    
    def _build_vocab(self, texts):
        """æ„å»ºè¯æ±‡è¡¨"""
        vocab = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        
        for text in texts:
            words = text.lower().split()
            for word in words:
                if word not in vocab:
                    vocab[word] = len(vocab)
        
        return vocab
    
    def _encode_text(self, text):
        """ç¼–ç æ–‡æœ¬"""
        words = text.lower().split()
        indices = [self.vocab.get(word, 1) for word in words]  # 1æ˜¯<UNK>
        
        # æ·»åŠ å¼€å§‹å’Œç»“æŸæ ‡è®°
        indices = [2] + indices + [3]  # 2æ˜¯<START>, 3æ˜¯<END>
        
        # æˆªæ–­æˆ–å¡«å……
        if len(indices) > self.max_text_length:
            indices = indices[:self.max_text_length]
        else:
            indices.extend([0] * (self.max_text_length - len(indices)))  # 0æ˜¯<PAD>
        
        return torch.tensor(indices, dtype=torch.long)
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        if isinstance(self.image_paths[idx], str):
            image = Image.open(self.image_paths[idx]).convert('RGB')
        else:
            # å¦‚æœæ˜¯numpyæ•°ç»„æˆ–tensor
            image = self.image_paths[idx]
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)
        
        if self.transform:
            image = self.transform(image)
        
        # ç¼–ç æ–‡æœ¬
        text_encoded = self._encode_text(self.texts[idx])
        
        # æ ‡ç­¾
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        
        return {
            'image': image,
            'text': text_encoded,
            'label': label
        }

class MultimodalModel(nn.Module):
    """å¤šæ¨¡æ€èåˆæ¨¡å‹"""
    
    def __init__(self, vocab_size, num_classes=2, image_feature_dim=512, text_feature_dim=256):
        super(MultimodalModel, self).__init__()
        
        # å›¾åƒç¼–ç å™¨ (ä½¿ç”¨é¢„è®­ç»ƒResNet)
        self.image_encoder = models.resnet18(pretrained=True)
        self.image_encoder.fc = nn.Linear(self.image_encoder.fc.in_features, image_feature_dim)
        
        # æ–‡æœ¬ç¼–ç å™¨ (LSTM)
        self.text_embedding = nn.Embedding(vocab_size, 128, padding_idx=0)
        self.text_lstm = nn.LSTM(128, text_feature_dim//2, batch_first=True, bidirectional=True)
        
        # å¤šæ¨¡æ€èåˆå±‚
        self.fusion_dim = image_feature_dim + text_feature_dim
        self.fusion_layer = nn.Sequential(
            nn.Linear(self.fusion_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(embed_dim=256, num_heads=8, batch_first=True)
        self.attention_norm = nn.LayerNorm(256)
    
    def forward(self, images, texts):
        # å›¾åƒç‰¹å¾æå–
        image_features = self.image_encoder(images)  # [batch_size, image_feature_dim]
        
        # æ–‡æœ¬ç‰¹å¾æå–
        text_embedded = self.text_embedding(texts)  # [batch_size, seq_len, 128]
        text_lstm_out, _ = self.text_lstm(text_embedded)  # [batch_size, seq_len, text_feature_dim]
        
        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èšåˆæ–‡æœ¬ç‰¹å¾
        text_attended, _ = self.attention(text_lstm_out, text_lstm_out, text_lstm_out)
        text_attended = self.attention_norm(text_attended + text_lstm_out)
        text_features = text_attended.mean(dim=1)  # [batch_size, text_feature_dim]
        
        # å¤šæ¨¡æ€ç‰¹å¾èåˆ
        fused_features = torch.cat([image_features, text_features], dim=1)
        
        # åˆ†ç±»
        output = self.fusion_layer(fused_features)
        
        return output

class MultimodalBaseline:
    """å¤šæ¨¡æ€Baselineç±»"""
    
    def __init__(self, num_classes=2, image_size=224, max_text_length=50):
        """
        åˆå§‹åŒ–
        num_classes: ç±»åˆ«æ•°é‡
        image_size: å›¾åƒå¤§å°
        max_text_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
        """
        self.num_classes = num_classes
        self.image_size = image_size
        self.max_text_length = max_text_length
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def create_sample_dataset(self):
        """åˆ›å»ºç¤ºä¾‹å¤šæ¨¡æ€æ•°æ®é›†"""
        print("åˆ›å»ºç¤ºä¾‹å¤šæ¨¡æ€æ•°æ®é›†...")
        
        # ç”Ÿæˆåˆæˆå›¾åƒæ•°æ® (ç”¨éšæœºå›¾åƒä»£æ›¿çœŸå®å›¾åƒ)
        np.random.seed(42)
        n_samples = 1000
        
        images = []
        texts = []
        labels = []
        
        # ç±»åˆ«0: æ­£é¢æƒ…æ„Ÿçš„å›¾åƒ+æ–‡æœ¬
        positive_texts = [
            "beautiful sunset over the ocean",
            "happy family enjoying vacation",
            "delicious food at restaurant",
            "amazing mountain landscape view",
            "cute puppy playing in garden",
            "wonderful celebration with friends",
            "peaceful nature scene",
            "exciting adventure in forest",
            "lovely flowers in bloom",
            "joyful children playing together"
        ]
        
        # ç±»åˆ«1: è´Ÿé¢æƒ…æ„Ÿçš„å›¾åƒ+æ–‡æœ¬
        negative_texts = [
            "dark stormy weather approaching",
            "broken old abandoned building",
            "sad lonely person sitting",
            "polluted dirty city street",
            "dangerous wild animal hunting",
            "destroyed damaged car accident",
            "empty cold winter landscape",
            "scary horror movie scene",
            "sick injured animal suffering",
            "angry crowd protesting loudly"
        ]
        
        # ç”Ÿæˆæ­£é¢æ ·æœ¬
        for i in range(n_samples // 2):
            # ç”Ÿæˆéšæœºå›¾åƒ (æ¨¡æ‹Ÿæ˜äº®çš„æ­£é¢å›¾åƒ)
            image = np.random.randint(100, 255, (self.image_size, self.image_size, 3), dtype=np.uint8)
            images.append(image)
            
            # éšæœºé€‰æ‹©æ­£é¢æ–‡æœ¬
            text = random.choice(positive_texts)
            texts.append(text)
            
            labels.append(1)  # æ­£é¢æ ‡ç­¾
        
        # ç”Ÿæˆè´Ÿé¢æ ·æœ¬
        for i in range(n_samples // 2):
            # ç”Ÿæˆéšæœºå›¾åƒ (æ¨¡æ‹Ÿæš—æ·¡çš„è´Ÿé¢å›¾åƒ)
            image = np.random.randint(0, 150, (self.image_size, self.image_size, 3), dtype=np.uint8)
            images.append(image)
            
            # éšæœºé€‰æ‹©è´Ÿé¢æ–‡æœ¬
            text = random.choice(negative_texts)
            texts.append(text)
            
            labels.append(0)  # è´Ÿé¢æ ‡ç­¾
        
        # æ‰“ä¹±æ•°æ®
        combined = list(zip(images, texts, labels))
        random.shuffle(combined)
        images, texts, labels = zip(*combined)
        
        # åˆ†å‰²æ•°æ®
        split_idx = int(0.8 * len(images))
        
        self.train_images = images[:split_idx]
        self.train_texts = texts[:split_idx]
        self.train_labels = labels[:split_idx]
        
        self.test_images = images[split_idx:]
        self.test_texts = texts[split_idx:]
        self.test_labels = labels[split_idx:]
        
        print(f"æ•°æ®é›†åˆ›å»ºå®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(self.train_images)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(self.test_images)} æ ·æœ¬")
        print(f"  ç±»åˆ«æ•°: {self.num_classes}")
    
    def create_data_loaders(self, batch_size=16):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        # å›¾åƒé¢„å¤„ç†
        transform = transforms.Compose([
            transforms.Resize((self.image_size, self.image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = MultimodalDataset(
            self.train_images, self.train_texts, self.train_labels,
            transform=transform, max_text_length=self.max_text_length
        )
        
        test_dataset = MultimodalDataset(
            self.test_images, self.test_texts, self.test_labels,
            transform=transform, max_text_length=self.max_text_length
        )
        
        # ä¿å­˜è¯æ±‡è¡¨å¤§å°
        self.vocab_size = train_dataset.vocab_size
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
        print(f"  è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    def create_model(self):
        """åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹"""
        print("åˆ›å»ºå¤šæ¨¡æ€èåˆæ¨¡å‹...")
        
        self.model = MultimodalModel(
            vocab_size=self.vocab_size,
            num_classes=self.num_classes,
            image_feature_dim=512,
            text_feature_dim=256
        )
        
        self.model = self.model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    def train_model(self, epochs=10, learning_rate=0.001):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"å¼€å§‹è®­ç»ƒï¼Œepochs: {epochs}")
        
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            running_loss = 0.0
            correct = 0
            total = 0
            
            for batch_idx, batch in enumerate(self.train_loader):
                images = batch['image'].to(self.device)
                texts = batch['text'].to(self.device)
                labels = batch['label'].to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(images, texts)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                if batch_idx % 10 == 0:
                    print(f'Epoch: {epoch+1}/{epochs}, Batch: {batch_idx}, '
                          f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')
            
            train_loss = running_loss / len(self.train_loader)
            train_acc = 100. * correct / total
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self.evaluate_model()
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            
            scheduler.step()
            
            print(f'Epoch {epoch+1}/{epochs}:')
            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
            print('-' * 50)
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        test_loss = 0
        correct = 0
        total = 0
        
        criterion = nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for batch in self.test_loader:
                images = batch['image'].to(self.device)
                texts = batch['text'].to(self.device)
                labels = batch['label'].to(self.device)
                
                outputs = self.model(images, texts)
                test_loss += criterion(outputs, labels).item()
                
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        test_loss /= len(self.test_loader)
        accuracy = 100. * correct / total
        
        return test_loss, accuracy
    
    def visualize_results(self):
        """å¯è§†åŒ–è®­ç»ƒç»“æœ"""
        plt.figure(figsize=(15, 10))
        
        # è®­ç»ƒå†å²
        plt.subplot(2, 3, 1)
        plt.plot(self.history['train_loss'], label='Train Loss')
        plt.plot(self.history['val_loss'], label='Val Loss')
        plt.title('è®­ç»ƒæŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(2, 3, 2)
        plt.plot(self.history['train_acc'], label='Train Acc')
        plt.plot(self.history['val_acc'], label='Val Acc')
        plt.title('è®­ç»ƒå‡†ç¡®ç‡')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.legend()
        plt.grid(True)
        
        # æ˜¾ç¤ºä¸€äº›æµ‹è¯•æ ·æœ¬
        plt.subplot(2, 3, 3)
        self._show_sample_predictions()
        
        plt.tight_layout()
        plt.show()
    
    def _show_sample_predictions(self):
        """æ˜¾ç¤ºæ ·æœ¬é¢„æµ‹ç»“æœ"""
        self.model.eval()
        
        # è·å–ä¸€ä¸ªbatchçš„æ•°æ®
        data_iter = iter(self.test_loader)
        batch = next(data_iter)
        
        images = batch['image'].to(self.device)
        texts = batch['text'].to(self.device)
        labels = batch['label'].to(self.device)
        
        with torch.no_grad():
            outputs = self.model(images, texts)
            _, predicted = torch.max(outputs, 1)
        
        # æ˜¾ç¤ºå‰4ä¸ªæ ·æœ¬
        fig, axes = plt.subplots(2, 2, figsize=(10, 8))
        axes = axes.ravel()
        
        for i in range(min(4, len(images))):
            img = images[i].cpu()
            # åæ ‡å‡†åŒ–
            img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            img = img + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            img = torch.clamp(img, 0, 1)
            
            axes[i].imshow(img.permute(1, 2, 0))
            
            # è§£ç æ–‡æœ¬ (ç®€åŒ–æ˜¾ç¤º)
            text_tokens = texts[i].cpu().numpy()
            text_preview = f"Text tokens: {text_tokens[:5]}..."
            
            sentiment_true = "Positive" if labels[i].item() == 1 else "Negative"
            sentiment_pred = "Positive" if predicted[i].item() == 1 else "Negative"
            
            axes[i].set_title(f'True: {sentiment_true}\nPred: {sentiment_pred}\n{text_preview}')
            axes[i].axis('off')
        
        plt.tight_layout()
    
    def run_baseline(self, epochs=5, learning_rate=0.001, batch_size=16):
        """è¿è¡Œå®Œæ•´çš„baselineæµç¨‹"""
        print("ğŸš€ å¼€å§‹å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ Baseline")
        print("=" * 60)
        
        # 1. åˆ›å»ºæ•°æ®é›†
        self.create_sample_dataset()
        
        # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.create_data_loaders(batch_size)
        
        # 3. åˆ›å»ºæ¨¡å‹
        self.create_model()
        
        # 4. è®­ç»ƒæ¨¡å‹
        self.train_model(epochs, learning_rate)
        
        # 5. è¯„ä¼°æ¨¡å‹
        final_loss, final_acc = self.evaluate_model()
        
        # 6. å¯è§†åŒ–ç»“æœ
        self.visualize_results()
        
        print(f"\nğŸ‰ Baselineå®Œæˆï¼")
        print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.2f}%")
        
        return self.model, final_acc

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    print("=" * 60)
    print("å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ Baselineç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºbaselineå®ä¾‹
    multimodal_baseline = MultimodalBaseline(
        num_classes=2,
        image_size=64,  # ä½¿ç”¨è¾ƒå°çš„å›¾åƒä»¥ä¾¿å¿«é€Ÿè®­ç»ƒ
        max_text_length=20
    )
    
    # è¿è¡Œbaseline
    model, accuracy = multimodal_baseline.run_baseline(
        epochs=3,  # å°‘é‡epochsç”¨äºæ¼”ç¤º
        learning_rate=0.001,
        batch_size=8
    )
    
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. è°ƒæ•´å›¾åƒå¤§å°: MultimodalBaseline(image_size=224)")
    print("2. è°ƒæ•´æ–‡æœ¬é•¿åº¦: MultimodalBaseline(max_text_length=100)")
    print("3. è‡ªå®šä¹‰æ•°æ®é›†: æ›¿æ¢create_sample_datasetæ–¹æ³•")
    print("4. æ¨¡å‹æ¶æ„: å›¾åƒResNet + æ–‡æœ¬LSTM + æ³¨æ„åŠ›èåˆ")

if __name__ == "__main__":
    main()
