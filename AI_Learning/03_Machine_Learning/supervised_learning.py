"""
ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°
ä»é›¶å¼€å§‹å®ç°ä¸»è¦çš„ç›‘ç£å­¦ä¹ ç®—æ³•
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score

class SupervisedLearning:
    """ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°ç±»"""
    
    def __init__(self):
        self.algorithms_implemented = []
        print("ğŸ¤– ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°ç³»ç»Ÿ")
        print("=" * 50)
    
    def linear_regression_implementation(self):
        """çº¿æ€§å›å½’å®ç°"""
        print("ğŸ“ˆ çº¿æ€§å›å½’ç®—æ³•å®ç°")
        print("=" * 30)
        
        class LinearRegression:
            def __init__(self, learning_rate=0.01, max_iterations=1000):
                self.learning_rate = learning_rate
                self.max_iterations = max_iterations
                self.weights = None
                self.bias = None
                self.cost_history = []
            
            def fit(self, X, y):
                n_samples, n_features = X.shape
                self.weights = np.zeros(n_features)
                self.bias = 0
                
                for i in range(self.max_iterations):
                    # å‰å‘ä¼ æ’­
                    y_pred = self.predict(X)
                    
                    # è®¡ç®—æŸå¤±
                    cost = np.mean((y - y_pred) ** 2)
                    self.cost_history.append(cost)
                    
                    # è®¡ç®—æ¢¯åº¦
                    dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
                    db = (1/n_samples) * np.sum(y_pred - y)
                    
                    # æ›´æ–°å‚æ•°
                    self.weights -= self.learning_rate * dw
                    self.bias -= self.learning_rate * db
            
            def predict(self, X):
                return np.dot(X, self.weights) + self.bias
        
        # ç”Ÿæˆå›å½’æ•°æ®
        X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # è®­ç»ƒæ¨¡å‹
        model = LinearRegression(learning_rate=0.01, max_iterations=1000)
        model.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        print(f"å‡æ–¹è¯¯å·®: {mse:.4f}")
        print(f"RÂ²å¾—åˆ†: {r2:.4f}")
        print(f"æƒé‡: {model.weights[0]:.4f}")
        print(f"åç½®: {model.bias:.4f}")
        
        # å¯è§†åŒ–
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 3, 1)
        plt.scatter(X_test, y_test, alpha=0.6, label='çœŸå®å€¼')
        plt.scatter(X_test, y_pred, alpha=0.6, label='é¢„æµ‹å€¼')
        plt.xlabel('ç‰¹å¾')
        plt.ylabel('ç›®æ ‡å€¼')
        plt.title('çº¿æ€§å›å½’é¢„æµ‹ç»“æœ')
        plt.legend()
        
        plt.subplot(1, 3, 2)
        plt.plot(model.cost_history)
        plt.title('æŸå¤±å‡½æ•°å˜åŒ–')
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('å‡æ–¹è¯¯å·®')
        
        plt.subplot(1, 3, 3)
        plt.scatter(y_test, y_pred, alpha=0.6)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        plt.xlabel('çœŸå®å€¼')
        plt.ylabel('é¢„æµ‹å€¼')
        plt.title('é¢„æµ‹vsçœŸå®')
        
        plt.tight_layout()
        plt.show()
        
        self.algorithms_implemented.append("çº¿æ€§å›å½’")
    
    def logistic_regression_implementation(self):
        """é€»è¾‘å›å½’å®ç°"""
        print("\nğŸ¯ é€»è¾‘å›å½’ç®—æ³•å®ç°")
        print("=" * 30)
        
        class LogisticRegression:
            def __init__(self, learning_rate=0.01, max_iterations=1000):
                self.learning_rate = learning_rate
                self.max_iterations = max_iterations
                self.weights = None
                self.bias = None
                self.cost_history = []
            
            def sigmoid(self, z):
                z = np.clip(z, -500, 500)
                return 1 / (1 + np.exp(-z))
            
            def fit(self, X, y):
                n_samples, n_features = X.shape
                self.weights = np.zeros(n_features)
                self.bias = 0
                
                for i in range(self.max_iterations):
                    # å‰å‘ä¼ æ’­
                    linear_pred = np.dot(X, self.weights) + self.bias
                    predictions = self.sigmoid(linear_pred)
                    
                    # è®¡ç®—æŸå¤±
                    cost = self.compute_cost(y, predictions)
                    self.cost_history.append(cost)
                    
                    # è®¡ç®—æ¢¯åº¦
                    dw = (1/n_samples) * np.dot(X.T, (predictions - y))
                    db = (1/n_samples) * np.sum(predictions - y)
                    
                    # æ›´æ–°å‚æ•°
                    self.weights -= self.learning_rate * dw
                    self.bias -= self.learning_rate * db
            
            def compute_cost(self, y_true, y_pred):
                y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)
                return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
            
            def predict(self, X):
                linear_pred = np.dot(X, self.weights) + self.bias
                y_pred = self.sigmoid(linear_pred)
                return [0 if y <= 0.5 else 1 for y in y_pred]
            
            def predict_proba(self, X):
                linear_pred = np.dot(X, self.weights) + self.bias
                return self.sigmoid(linear_pred)
        
        # ç”Ÿæˆåˆ†ç±»æ•°æ®
        X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, 
                                 n_informative=2, n_clusters_per_class=1, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # è®­ç»ƒæ¨¡å‹
        model = LogisticRegression(learning_rate=0.1, max_iterations=1000)
        model.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"æƒé‡: {model.weights}")
        print(f"åç½®: {model.bias:.4f}")
        
        # å¯è§†åŒ–
        plt.figure(figsize=(15, 5))
        
        # çœŸå®åˆ†ç±»
        plt.subplot(1, 3, 1)
        scatter = plt.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], c=y_test, cmap='viridis', alpha=0.6)
        plt.colorbar(scatter)
        plt.title('çœŸå®åˆ†ç±»')
        plt.xlabel('ç‰¹å¾1')
        plt.ylabel('ç‰¹å¾2')
        
        # é¢„æµ‹æ¦‚ç‡
        plt.subplot(1, 3, 2)
        scatter = plt.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], c=y_pred_proba, cmap='viridis', alpha=0.6)
        plt.colorbar(scatter)
        plt.title('é¢„æµ‹æ¦‚ç‡')
        plt.xlabel('ç‰¹å¾1')
        plt.ylabel('ç‰¹å¾2')
        
        # æŸå¤±å‡½æ•°å˜åŒ–
        plt.subplot(1, 3, 3)
        plt.plot(model.cost_history)
        plt.title('æŸå¤±å‡½æ•°å˜åŒ–')
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('äº¤å‰ç†µæŸå¤±')
        
        plt.tight_layout()
        plt.show()
        
        self.algorithms_implemented.append("é€»è¾‘å›å½’")
    
    def knn_implementation(self):
        """Kè¿‘é‚»ç®—æ³•å®ç°"""
        print("\nğŸ‘¥ Kè¿‘é‚»ç®—æ³•å®ç°")
        print("=" * 30)
        
        class KNearestNeighbors:
            def __init__(self, k=3):
                self.k = k
                self.X_train = None
                self.y_train = None
            
            def fit(self, X, y):
                self.X_train = X
                self.y_train = y
            
            def euclidean_distance(self, x1, x2):
                return np.sqrt(np.sum((x1 - x2) ** 2))
            
            def predict(self, X):
                predictions = []
                for sample in X:
                    # è®¡ç®—åˆ°æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»
                    distances = []
                    for i, train_sample in enumerate(self.X_train):
                        distance = self.euclidean_distance(sample, train_sample)
                        distances.append((distance, self.y_train[i]))
                    
                    # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»
                    distances.sort(key=lambda x: x[0])
                    k_nearest = distances[:self.k]
                    
                    # æŠ•ç¥¨å†³å®šç±»åˆ«
                    k_nearest_labels = [label for _, label in k_nearest]
                    prediction = max(set(k_nearest_labels), key=k_nearest_labels.count)
                    predictions.append(prediction)
                
                return predictions
        
        # ç”Ÿæˆåˆ†ç±»æ•°æ®
        X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, 
                                 n_informative=2, n_clusters_per_class=1, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # æµ‹è¯•ä¸åŒçš„kå€¼
        k_values = [1, 3, 5, 7, 9]
        accuracies = []
        
        for k in k_values:
            knn = KNearestNeighbors(k=k)
            knn.fit(X_train_scaled, y_train)
            y_pred = knn.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            accuracies.append(accuracy)
            print(f"k={k}, å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # å¯è§†åŒ–kå€¼å¯¹å‡†ç¡®ç‡çš„å½±å“
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)
        plt.xlabel('kå€¼')
        plt.ylabel('å‡†ç¡®ç‡')
        plt.title('kå€¼å¯¹KNNå‡†ç¡®ç‡çš„å½±å“')
        plt.grid(True, alpha=0.3)
        
        # å¯è§†åŒ–æœ€ä½³kå€¼çš„ç»“æœ
        best_k = k_values[np.argmax(accuracies)]
        knn_best = KNearestNeighbors(k=best_k)
        knn_best.fit(X_train_scaled, y_train)
        
        plt.subplot(1, 2, 2)
        scatter = plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], 
                            c=y_train, cmap='viridis', alpha=0.6)
        plt.xlabel('ç‰¹å¾1 (æ ‡å‡†åŒ–)')
        plt.ylabel('ç‰¹å¾2 (æ ‡å‡†åŒ–)')
        plt.title(f'KNNåˆ†ç±» (k={best_k})')
        plt.colorbar(scatter)
        
        plt.tight_layout()
        plt.show()
        
        self.algorithms_implemented.append("Kè¿‘é‚»")
    
    def run_all_algorithms(self):
        """è¿è¡Œæ‰€æœ‰ç›‘ç£å­¦ä¹ ç®—æ³•"""
        print("ğŸ“ ç›‘ç£å­¦ä¹ ç®—æ³•å®Œæ•´å®ç°")
        print("=" * 60)
        
        self.linear_regression_implementation()
        self.logistic_regression_implementation()
        self.knn_implementation()
        
        print(f"\nğŸ‰ ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°å®Œæˆï¼")
        print(f"å·²å®ç°çš„ç®—æ³•: {', '.join(self.algorithms_implemented)}")
        
        print(f"\nğŸ“š ç®—æ³•æ€»ç»“:")
        print("1. çº¿æ€§å›å½’ - è¿ç»­å€¼é¢„æµ‹ï¼Œæœ€å°äºŒä¹˜æ³•")
        print("2. é€»è¾‘å›å½’ - äºŒåˆ†ç±»ï¼Œsigmoidå‡½æ•°")
        print("3. Kè¿‘é‚» - åŸºäºè·ç¦»çš„åˆ†ç±»ï¼Œæ‡’æƒ°å­¦ä¹ ")

def main():
    """ä¸»å‡½æ•°"""
    supervised = SupervisedLearning()
    supervised.run_all_algorithms()
    
    print("\nğŸ’¡ ç®—æ³•é€‰æ‹©æŒ‡å—:")
    print("1. çº¿æ€§å›å½’ - çº¿æ€§å…³ç³»ï¼Œå¯è§£é‡Šæ€§å¼º")
    print("2. é€»è¾‘å›å½’ - çº¿æ€§åˆ†ç±»è¾¹ç•Œï¼Œæ¦‚ç‡è¾“å‡º")
    print("3. Kè¿‘é‚» - éå‚æ•°ï¼Œå±€éƒ¨æ¨¡å¼ï¼Œéœ€è¦å¤§é‡æ•°æ®")

if __name__ == "__main__":
    main()
