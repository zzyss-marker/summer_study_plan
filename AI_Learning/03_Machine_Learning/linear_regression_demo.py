"""
çº¿æ€§å›å½’ç®—æ³•å®ç°æ¼”ç¤º
ä»é›¶å¼€å§‹å®ç°çº¿æ€§å›å½’ï¼Œç†è§£æ¢¯åº¦ä¸‹é™åŸç†
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # æ”¯æŒä¸­æ–‡æ˜¾ç¤º

class LinearRegression:
    """ä»é›¶å®ç°çº¿æ€§å›å½’"""
    
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def fit(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        # åˆå§‹åŒ–å‚æ•°
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # æ¢¯åº¦ä¸‹é™
        for i in range(self.max_iterations):
            # å‰å‘ä¼ æ’­
            y_pred = self.predict(X)
            
            # è®¡ç®—æŸå¤±
            cost = self.compute_cost(y, y_pred)
            self.cost_history.append(cost)
            
            # è®¡ç®—æ¢¯åº¦
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            # æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X):
        """é¢„æµ‹"""
        return np.dot(X, self.weights) + self.bias
    
    def compute_cost(self, y_true, y_pred):
        """è®¡ç®—å‡æ–¹è¯¯å·®æŸå¤±"""
        return np.mean((y_true - y_pred) ** 2)
    
    def plot_cost_history(self):
        """ç»˜åˆ¶æŸå¤±å‡½æ•°å˜åŒ–"""
        plt.figure(figsize=(10, 6))
        plt.plot(self.cost_history)
        plt.title('æŸå¤±å‡½æ•°å˜åŒ–')
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('å‡æ–¹è¯¯å·®')
        plt.grid(True)
        plt.show()

def demo_linear_regression():
    """çº¿æ€§å›å½’æ¼”ç¤º"""
    print("ğŸ¤– çº¿æ€§å›å½’ç®—æ³•æ¼”ç¤º")
    print("=" * 50)
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # è®­ç»ƒæ¨¡å‹
    model = LinearRegression(learning_rate=0.01, max_iterations=1000)
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    
    # è¯„ä¼°
    mse = np.mean((y_test - y_pred) ** 2)
    r2 = 1 - (np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))
    
    print(f"å‡æ–¹è¯¯å·®: {mse:.4f}")
    print(f"RÂ²å¾—åˆ†: {r2:.4f}")
    print(f"æƒé‡: {model.weights[0]:.4f}")
    print(f"åç½®: {model.bias:.4f}")
    
    # å¯è§†åŒ–ç»“æœ
    plt.figure(figsize=(15, 5))
    
    # è®­ç»ƒæ•°æ®
    plt.subplot(1, 3, 1)
    plt.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®')
    plt.xlabel('ç‰¹å¾')
    plt.ylabel('ç›®æ ‡å€¼')
    plt.title('è®­ç»ƒæ•°æ®åˆ†å¸ƒ')
    plt.legend()
    
    # é¢„æµ‹ç»“æœ
    plt.subplot(1, 3, 2)
    plt.scatter(X_test, y_test, alpha=0.6, label='çœŸå®å€¼')
    plt.scatter(X_test, y_pred, alpha=0.6, label='é¢„æµ‹å€¼')
    plt.xlabel('ç‰¹å¾')
    plt.ylabel('ç›®æ ‡å€¼')
    plt.title('é¢„æµ‹ç»“æœå¯¹æ¯”')
    plt.legend()
    
    # æŸå¤±å‡½æ•°
    plt.subplot(1, 3, 3)
    plt.plot(model.cost_history)
    plt.title('æŸå¤±å‡½æ•°å˜åŒ–')
    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('å‡æ–¹è¯¯å·®')
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    demo_linear_regression()
    print("\nğŸ‰ çº¿æ€§å›å½’å­¦ä¹ å®Œæˆï¼")
    print("ğŸ’¡ å­¦ä¹ è¦ç‚¹ï¼š")
    print("   - ç†è§£æ¢¯åº¦ä¸‹é™ç®—æ³•åŸç†")
    print("   - æŒæ¡æŸå¤±å‡½æ•°çš„è®¡ç®—")
    print("   - å­¦ä¼šæ¨¡å‹è¯„ä¼°æŒ‡æ ‡")
