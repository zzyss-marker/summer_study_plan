"""
æ— ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°
ä»é›¶å¼€å§‹å®ç°èšç±»å’Œé™ç»´ç®—æ³•
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs, make_circles
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei']

class UnsupervisedLearning:
    """æ— ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°ç±»"""
    
    def __init__(self):
        self.algorithms_implemented = []
        print("ğŸ” æ— ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°ç³»ç»Ÿ")
        print("=" * 50)
    
    def kmeans_implementation(self):
        """K-meansèšç±»ç®—æ³•å®ç°"""
        print("ğŸ¯ K-meansèšç±»ç®—æ³•å®ç°")
        print("=" * 30)
        
        class KMeans:
            def __init__(self, k=3, max_iters=100, random_state=42):
                self.k = k
                self.max_iters = max_iters
                self.random_state = random_state
                
            def fit(self, X):
                np.random.seed(self.random_state)
                n_samples, n_features = X.shape
                
                # éšæœºåˆå§‹åŒ–èšç±»ä¸­å¿ƒ
                self.centroids = X[np.random.choice(n_samples, self.k, replace=False)]
                
                self.history = {'centroids': [self.centroids.copy()], 'labels': []}
                
                for iteration in range(self.max_iters):
                    # åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
                    distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
                    labels = np.argmin(distances, axis=0)
                    
                    self.history['labels'].append(labels.copy())
                    
                    # æ›´æ–°èšç±»ä¸­å¿ƒ
                    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(self.k)])
                    
                    # æ£€æŸ¥æ”¶æ•›
                    if np.allclose(self.centroids, new_centroids):
                        print(f"K-meansåœ¨ç¬¬{iteration+1}æ¬¡è¿­ä»£åæ”¶æ•›")
                        break
                    
                    self.centroids = new_centroids
                    self.history['centroids'].append(self.centroids.copy())
                
                self.labels_ = labels
                return self
            
            def predict(self, X):
                distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
                return np.argmin(distances, axis=0)
            
            def inertia(self, X):
                """è®¡ç®—ç°‡å†…å¹³æ–¹å’Œ"""
                total_inertia = 0
                for i in range(self.k):
                    cluster_points = X[self.labels_ == i]
                    if len(cluster_points) > 0:
                        total_inertia += np.sum((cluster_points - self.centroids[i])**2)
                return total_inertia
        
        # ç”Ÿæˆèšç±»æ•°æ®
        X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, 
                              random_state=42)
        
        # è®­ç»ƒK-means
        kmeans = KMeans(k=4, random_state=42)
        kmeans.fit(X)
        
        # è¯„ä¼°ç»“æœ
        silhouette_avg = silhouette_score(X, kmeans.labels_)
        ari_score = adjusted_rand_score(y_true, kmeans.labels_)
        inertia = kmeans.inertia(X)
        
        print(f"è½®å»“ç³»æ•°: {silhouette_avg:.4f}")
        print(f"è°ƒæ•´å…°å¾·æŒ‡æ•°: {ari_score:.4f}")
        print(f"ç°‡å†…å¹³æ–¹å’Œ: {inertia:.2f}")
        
        # è‚˜éƒ¨æ³•åˆ™ - ç¡®å®šæœ€ä½³kå€¼
        print(f"\nè‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä½³kå€¼:")
        k_range = range(1, 11)
        inertias = []
        
        for k in k_range:
            kmeans_k = KMeans(k=k, random_state=42)
            kmeans_k.fit(X)
            inertias.append(kmeans_k.inertia(X))
        
        # å¯è§†åŒ–ç»“æœ
        plt.figure(figsize=(15, 10))
        
        # K-meansèšç±»ç»“æœ
        plt.subplot(2, 3, 1)
        colors = ['red', 'blue', 'green', 'purple', 'orange']
        for i in range(kmeans.k):
            cluster_points = X[kmeans.labels_ == i]
            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], 
                       c=colors[i], alpha=0.6, label=f'ç°‡ {i+1}')
        
        plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], 
                   c='black', marker='x', s=200, linewidths=3, label='èšç±»ä¸­å¿ƒ')
        plt.title('K-meansèšç±»ç»“æœ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # çœŸå®æ ‡ç­¾
        plt.subplot(2, 3, 2)
        plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.6)
        plt.title('çœŸå®èšç±»')
        plt.grid(True, alpha=0.3)
        
        # è‚˜éƒ¨æ³•åˆ™
        plt.subplot(2, 3, 3)
        plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)
        plt.xlabel('èšç±»æ•° k')
        plt.ylabel('ç°‡å†…å¹³æ–¹å’Œ')
        plt.title('è‚˜éƒ¨æ³•åˆ™')
        plt.grid(True, alpha=0.3)
        
        # K-meansæ”¶æ•›è¿‡ç¨‹
        plt.subplot(2, 3, 4)
        plt.scatter(X[:, 0], X[:, 1], c='lightgray', alpha=0.5)
        
        # æ˜¾ç¤ºå‰å‡ æ¬¡è¿­ä»£çš„èšç±»ä¸­å¿ƒ
        for i, centroids in enumerate(kmeans.history['centroids'][:5]):
            alpha = 0.3 + 0.7 * i / 4  # é€æ˜åº¦é€’å¢
            plt.scatter(centroids[:, 0], centroids[:, 1], 
                       c='red', marker='x', s=100, alpha=alpha, 
                       label=f'è¿­ä»£ {i+1}' if i < 3 else None)
        
        plt.title('K-meansæ”¶æ•›è¿‡ç¨‹')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # è½®å»“åˆ†æ
        plt.subplot(2, 3, 5)
        from sklearn.metrics import silhouette_samples
        silhouette_vals = silhouette_samples(X, kmeans.labels_)
        
        y_lower = 10
        for i in range(kmeans.k):
            cluster_silhouette_vals = silhouette_vals[kmeans.labels_ == i]
            cluster_silhouette_vals.sort()
            
            size_cluster_i = cluster_silhouette_vals.shape[0]
            y_upper = y_lower + size_cluster_i
            
            color = colors[i]
            plt.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,
                             facecolor=color, edgecolor=color, alpha=0.7)
            
            plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
            y_lower = y_upper + 10
        
        plt.axvline(x=silhouette_avg, color="red", linestyle="--", 
                   label=f'å¹³å‡è½®å»“ç³»æ•° = {silhouette_avg:.3f}')
        plt.xlabel('è½®å»“ç³»æ•°')
        plt.ylabel('ç°‡æ ‡ç­¾')
        plt.title('è½®å»“åˆ†æ')
        plt.legend()
        
        # ä¸åŒåˆå§‹åŒ–çš„æ¯”è¾ƒ
        plt.subplot(2, 3, 6)
        
        # å¤šæ¬¡è¿è¡ŒK-meansï¼Œæ¯”è¾ƒç»“æœç¨³å®šæ€§
        inertias_multiple = []
        for seed in range(10):
            kmeans_temp = KMeans(k=4, random_state=seed)
            kmeans_temp.fit(X)
            inertias_multiple.append(kmeans_temp.inertia(X))
        
        plt.bar(range(10), inertias_multiple, alpha=0.7)
        plt.xlabel('éšæœºç§å­')
        plt.ylabel('ç°‡å†…å¹³æ–¹å’Œ')
        plt.title('ä¸åŒåˆå§‹åŒ–çš„ç»“æœ')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        self.algorithms_implemented.append("K-meansèšç±»")
    
    def hierarchical_clustering_implementation(self):
        """å±‚æ¬¡èšç±»ç®—æ³•å®ç°"""
        print("\nğŸŒ³ å±‚æ¬¡èšç±»ç®—æ³•å®ç°")
        print("=" * 30)
        
        class HierarchicalClustering:
            def __init__(self, linkage='single'):
                self.linkage = linkage  # 'single', 'complete', 'average'
                
            def fit(self, X):
                n_samples = X.shape[0]
                
                # è®¡ç®—è·ç¦»çŸ©é˜µ
                self.distance_matrix = self._compute_distance_matrix(X)
                
                # åˆå§‹åŒ–ï¼šæ¯ä¸ªç‚¹ä¸ºä¸€ä¸ªç°‡
                clusters = [[i] for i in range(n_samples)]
                self.merge_history = []
                
                while len(clusters) > 1:
                    # æ‰¾åˆ°æœ€è¿‘çš„ä¸¤ä¸ªç°‡
                    min_dist = float('inf')
                    merge_i, merge_j = -1, -1
                    
                    for i in range(len(clusters)):
                        for j in range(i + 1, len(clusters)):
                            dist = self._cluster_distance(clusters[i], clusters[j])
                            if dist < min_dist:
                                min_dist = dist
                                merge_i, merge_j = i, j
                    
                    # åˆå¹¶ç°‡
                    new_cluster = clusters[merge_i] + clusters[merge_j]
                    self.merge_history.append((clusters[merge_i].copy(), 
                                             clusters[merge_j].copy(), min_dist))
                    
                    # æ›´æ–°ç°‡åˆ—è¡¨
                    clusters = [clusters[k] for k in range(len(clusters)) 
                               if k != merge_i and k != merge_j] + [new_cluster]
                
                return self
            
            def _compute_distance_matrix(self, X):
                n_samples = X.shape[0]
                dist_matrix = np.zeros((n_samples, n_samples))
                
                for i in range(n_samples):
                    for j in range(i + 1, n_samples):
                        dist = np.sqrt(np.sum((X[i] - X[j])**2))
                        dist_matrix[i, j] = dist_matrix[j, i] = dist
                
                return dist_matrix
            
            def _cluster_distance(self, cluster1, cluster2):
                distances = []
                for i in cluster1:
                    for j in cluster2:
                        distances.append(self.distance_matrix[i, j])
                
                if self.linkage == 'single':
                    return min(distances)  # å•é“¾æ¥
                elif self.linkage == 'complete':
                    return max(distances)  # å…¨é“¾æ¥
                elif self.linkage == 'average':
                    return np.mean(distances)  # å¹³å‡é“¾æ¥
            
            def get_clusters(self, n_clusters):
                """è·å–æŒ‡å®šæ•°é‡çš„ç°‡"""
                if n_clusters <= 0:
                    return []
                
                # ä»åˆå¹¶å†å²ä¸­é‡å»ºç°‡
                n_samples = len(self.merge_history) + 1
                clusters = [[i] for i in range(n_samples)]
                
                merges_to_do = n_samples - n_clusters
                for i in range(merges_to_do):
                    cluster1, cluster2, _ = self.merge_history[i]
                    
                    # æ‰¾åˆ°å¯¹åº”çš„ç°‡å¹¶åˆå¹¶
                    idx1 = idx2 = -1
                    for j, cluster in enumerate(clusters):
                        if set(cluster) == set(cluster1):
                            idx1 = j
                        elif set(cluster) == set(cluster2):
                            idx2 = j
                    
                    if idx1 != -1 and idx2 != -1:
                        new_cluster = clusters[idx1] + clusters[idx2]
                        clusters = [clusters[k] for k in range(len(clusters)) 
                                   if k != idx1 and k != idx2] + [new_cluster]
                
                return clusters
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        X_hier, _ = make_blobs(n_samples=50, centers=3, cluster_std=1.0, 
                              random_state=42)
        
        # æµ‹è¯•ä¸åŒçš„é“¾æ¥æ–¹æ³•
        linkage_methods = ['single', 'complete', 'average']
        
        plt.figure(figsize=(15, 5))
        
        for i, linkage in enumerate(linkage_methods):
            hc = HierarchicalClustering(linkage=linkage)
            hc.fit(X_hier)
            
            # è·å–3ä¸ªç°‡
            clusters = hc.get_clusters(3)
            
            # åˆ›å»ºæ ‡ç­¾æ•°ç»„
            labels = np.zeros(len(X_hier))
            for cluster_id, cluster in enumerate(clusters):
                for point_id in cluster:
                    labels[point_id] = cluster_id
            
            plt.subplot(1, 3, i + 1)
            plt.scatter(X_hier[:, 0], X_hier[:, 1], c=labels, cmap='viridis', alpha=0.7)
            plt.title(f'{linkage.capitalize()} Linkage')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        print("å±‚æ¬¡èšç±»ç‰¹ç‚¹:")
        print("â€¢ Single Linkage: å®¹æ˜“äº§ç”Ÿé“¾çŠ¶ç°‡")
        print("â€¢ Complete Linkage: å€¾å‘äºäº§ç”Ÿç´§å‡‘çš„çƒçŠ¶ç°‡")
        print("â€¢ Average Linkage: ä»‹äºä¸¤è€…ä¹‹é—´")
        
        self.algorithms_implemented.append("å±‚æ¬¡èšç±»")
    
    def pca_implementation(self):
        """ä¸»æˆåˆ†åˆ†æ(PCA)å®ç°"""
        print("\nğŸ“Š ä¸»æˆåˆ†åˆ†æ(PCA)å®ç°")
        print("=" * 30)
        
        class PCA:
            def __init__(self, n_components=2):
                self.n_components = n_components
                
            def fit(self, X):
                # æ•°æ®ä¸­å¿ƒåŒ–
                self.mean_ = np.mean(X, axis=0)
                X_centered = X - self.mean_
                
                # è®¡ç®—åæ–¹å·®çŸ©é˜µ
                cov_matrix = np.cov(X_centered.T)
                
                # ç‰¹å¾å€¼åˆ†è§£
                eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
                
                # æŒ‰ç‰¹å¾å€¼å¤§å°æ’åº
                idx = np.argsort(eigenvalues)[::-1]
                self.eigenvalues_ = eigenvalues[idx]
                self.eigenvectors_ = eigenvectors[:, idx]
                
                # é€‰æ‹©å‰n_componentsä¸ªä¸»æˆåˆ†
                self.components_ = self.eigenvectors_[:, :self.n_components].T
                
                # è®¡ç®—æ–¹å·®è§£é‡Šæ¯”ä¾‹
                self.explained_variance_ratio_ = self.eigenvalues_ / np.sum(self.eigenvalues_)
                
                return self
            
            def transform(self, X):
                X_centered = X - self.mean_
                return np.dot(X_centered, self.components_.T)
            
            def fit_transform(self, X):
                return self.fit(X).transform(X)
            
            def inverse_transform(self, X_transformed):
                return np.dot(X_transformed, self.components_) + self.mean_
        
        # ç”Ÿæˆé«˜ç»´æ•°æ®
        np.random.seed(42)
        n_samples = 200
        
        # åˆ›å»ºç›¸å…³çš„é«˜ç»´æ•°æ®
        X_high = np.random.randn(n_samples, 5)
        # æ·»åŠ ç›¸å…³æ€§
        X_high[:, 1] = X_high[:, 0] + 0.5 * np.random.randn(n_samples)
        X_high[:, 2] = -X_high[:, 0] + 0.3 * np.random.randn(n_samples)
        X_high[:, 3] = 0.5 * X_high[:, 1] + 0.4 * np.random.randn(n_samples)
        X_high[:, 4] = np.random.randn(n_samples)  # ç‹¬ç«‹ç»´åº¦
        
        # åº”ç”¨PCA
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_high)
        
        print(f"åŸå§‹æ•°æ®ç»´åº¦: {X_high.shape}")
        print(f"PCAåç»´åº¦: {X_pca.shape}")
        print(f"ä¸»æˆåˆ†æ–¹å·®è§£é‡Šæ¯”ä¾‹: {pca.explained_variance_ratio_[:2]}")
        print(f"ç´¯è®¡æ–¹å·®è§£é‡Šæ¯”ä¾‹: {np.sum(pca.explained_variance_ratio_[:2]):.4f}")
        
        # å¯è§†åŒ–PCAç»“æœ
        plt.figure(figsize=(15, 10))
        
        # åŸå§‹æ•°æ®çš„å‰ä¸¤ä¸ªç»´åº¦
        plt.subplot(2, 3, 1)
        plt.scatter(X_high[:, 0], X_high[:, 1], alpha=0.6)
        plt.xlabel('ç‰¹å¾1')
        plt.ylabel('ç‰¹å¾2')
        plt.title('åŸå§‹æ•°æ® (å‰ä¸¤ä¸ªç»´åº¦)')
        plt.grid(True, alpha=0.3)
        
        # PCAé™ç»´ç»“æœ
        plt.subplot(2, 3, 2)
        plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6, color='red')
        plt.xlabel('ç¬¬ä¸€ä¸»æˆåˆ†')
        plt.ylabel('ç¬¬äºŒä¸»æˆåˆ†')
        plt.title('PCAé™ç»´ç»“æœ')
        plt.grid(True, alpha=0.3)
        
        # æ–¹å·®è§£é‡Šæ¯”ä¾‹
        plt.subplot(2, 3, 3)
        plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), 
                pca.explained_variance_ratio_, alpha=0.7)
        plt.xlabel('ä¸»æˆåˆ†')
        plt.ylabel('æ–¹å·®è§£é‡Šæ¯”ä¾‹')
        plt.title('å„ä¸»æˆåˆ†æ–¹å·®è§£é‡Šæ¯”ä¾‹')
        plt.grid(True, alpha=0.3)
        
        # ç´¯è®¡æ–¹å·®è§£é‡Šæ¯”ä¾‹
        plt.subplot(2, 3, 4)
        cumsum_var = np.cumsum(pca.explained_variance_ratio_)
        plt.plot(range(1, len(cumsum_var) + 1), cumsum_var, 'bo-', linewidth=2)
        plt.axhline(y=0.95, color='r', linestyle='--', label='95%é˜ˆå€¼')
        plt.xlabel('ä¸»æˆåˆ†æ•°é‡')
        plt.ylabel('ç´¯è®¡æ–¹å·®è§£é‡Šæ¯”ä¾‹')
        plt.title('ç´¯è®¡æ–¹å·®è§£é‡Šæ¯”ä¾‹')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ä¸»æˆåˆ†è½½è·å›¾
        plt.subplot(2, 3, 5)
        feature_names = [f'ç‰¹å¾{i+1}' for i in range(X_high.shape[1])]
        
        for i, feature in enumerate(feature_names):
            plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], 
                     head_width=0.05, head_length=0.05, fc='blue', ec='blue')
            plt.text(pca.components_[0, i] * 1.1, pca.components_[1, i] * 1.1, 
                    feature, fontsize=10)
        
        plt.xlim(-1, 1)
        plt.ylim(-1, 1)
        plt.xlabel('ç¬¬ä¸€ä¸»æˆåˆ†è½½è·')
        plt.ylabel('ç¬¬äºŒä¸»æˆåˆ†è½½è·')
        plt.title('ä¸»æˆåˆ†è½½è·å›¾')
        plt.grid(True, alpha=0.3)
        
        # é‡æ„è¯¯å·®åˆ†æ
        plt.subplot(2, 3, 6)
        n_components_range = range(1, X_high.shape[1] + 1)
        reconstruction_errors = []
        
        for n_comp in n_components_range:
            pca_temp = PCA(n_components=n_comp)
            X_transformed = pca_temp.fit_transform(X_high)
            X_reconstructed = pca_temp.inverse_transform(X_transformed)
            error = np.mean((X_high - X_reconstructed)**2)
            reconstruction_errors.append(error)
        
        plt.plot(n_components_range, reconstruction_errors, 'go-', linewidth=2)
        plt.xlabel('ä¸»æˆåˆ†æ•°é‡')
        plt.ylabel('é‡æ„è¯¯å·®')
        plt.title('é‡æ„è¯¯å·® vs ä¸»æˆåˆ†æ•°é‡')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        self.algorithms_implemented.append("ä¸»æˆåˆ†åˆ†æ")
    
    def dbscan_implementation(self):
        """DBSCANå¯†åº¦èšç±»å®ç°"""
        print("\nğŸ” DBSCANå¯†åº¦èšç±»å®ç°")
        print("=" * 30)
        
        class DBSCAN:
            def __init__(self, eps=0.5, min_samples=5):
                self.eps = eps
                self.min_samples = min_samples
                
            def fit(self, X):
                n_samples = X.shape[0]
                self.labels_ = np.full(n_samples, -1)  # -1è¡¨ç¤ºå™ªå£°ç‚¹
                
                cluster_id = 0
                visited = np.zeros(n_samples, dtype=bool)
                
                for i in range(n_samples):
                    if visited[i]:
                        continue
                    
                    visited[i] = True
                    neighbors = self._get_neighbors(X, i)
                    
                    if len(neighbors) < self.min_samples:
                        # æ ‡è®°ä¸ºå™ªå£°ç‚¹
                        self.labels_[i] = -1
                    else:
                        # å¼€å§‹æ–°ç°‡
                        self._expand_cluster(X, i, neighbors, cluster_id, visited)
                        cluster_id += 1
                
                return self
            
            def _get_neighbors(self, X, point_idx):
                distances = np.sqrt(np.sum((X - X[point_idx])**2, axis=1))
                return np.where(distances <= self.eps)[0]
            
            def _expand_cluster(self, X, point_idx, neighbors, cluster_id, visited):
                self.labels_[point_idx] = cluster_id
                
                i = 0
                while i < len(neighbors):
                    neighbor_idx = neighbors[i]
                    
                    if not visited[neighbor_idx]:
                        visited[neighbor_idx] = True
                        new_neighbors = self._get_neighbors(X, neighbor_idx)
                        
                        if len(new_neighbors) >= self.min_samples:
                            neighbors = np.concatenate([neighbors, new_neighbors])
                    
                    if self.labels_[neighbor_idx] == -1:
                        self.labels_[neighbor_idx] = cluster_id
                    
                    i += 1
        
        # ç”ŸæˆåŒ…å«å™ªå£°çš„æ•°æ®
        X_circles, _ = make_circles(n_samples=300, factor=0.3, noise=0.1, random_state=42)
        
        # æ·»åŠ ä¸€äº›å™ªå£°ç‚¹
        noise_points = np.random.uniform(-2, 2, (50, 2))
        X_with_noise = np.vstack([X_circles, noise_points])
        
        # åº”ç”¨DBSCAN
        dbscan = DBSCAN(eps=0.3, min_samples=10)
        dbscan.fit(X_with_noise)
        
        # ç»Ÿè®¡ç»“æœ
        n_clusters = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)
        n_noise = list(dbscan.labels_).count(-1)
        
        print(f"èšç±»æ•°é‡: {n_clusters}")
        print(f"å™ªå£°ç‚¹æ•°é‡: {n_noise}")
        
        # å¯è§†åŒ–DBSCANç»“æœ
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plt.scatter(X_with_noise[:, 0], X_with_noise[:, 1], alpha=0.6)
        plt.title('åŸå§‹æ•°æ®ï¼ˆåŒ…å«å™ªå£°ï¼‰')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 2, 2)
        unique_labels = set(dbscan.labels_)
        colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))
        
        for k, col in zip(unique_labels, colors):
            if k == -1:
                # å™ªå£°ç‚¹ç”¨é»‘è‰²è¡¨ç¤º
                col = 'black'
                marker = 'x'
                alpha = 0.5
                label = 'å™ªå£°'
            else:
                marker = 'o'
                alpha = 0.7
                label = f'ç°‡ {k}'
            
            class_member_mask = (dbscan.labels_ == k)
            xy = X_with_noise[class_member_mask]
            plt.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, 
                       alpha=alpha, s=50, label=label)
        
        plt.title(f'DBSCANèšç±»ç»“æœ\n(eps={dbscan.eps}, min_samples={dbscan.min_samples})')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        print("DBSCANç‰¹ç‚¹:")
        print("â€¢ å¯ä»¥å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡")
        print("â€¢ è‡ªåŠ¨ç¡®å®šç°‡çš„æ•°é‡")
        print("â€¢ èƒ½å¤Ÿè¯†åˆ«å™ªå£°ç‚¹")
        print("â€¢ å¯¹å‚æ•°epså’Œmin_samplesæ•æ„Ÿ")
        
        self.algorithms_implemented.append("DBSCANèšç±»")
    
    def run_all_algorithms(self):
        """è¿è¡Œæ‰€æœ‰æ— ç›‘ç£å­¦ä¹ ç®—æ³•"""
        print("ğŸ” æ— ç›‘ç£å­¦ä¹ ç®—æ³•å®Œæ•´å®ç°")
        print("=" * 60)
        
        self.kmeans_implementation()
        self.hierarchical_clustering_implementation()
        self.pca_implementation()
        self.dbscan_implementation()
        
        print(f"\nğŸ‰ æ— ç›‘ç£å­¦ä¹ ç®—æ³•å®ç°å®Œæˆï¼")
        print(f"å·²å®ç°çš„ç®—æ³•: {', '.join(self.algorithms_implemented)}")
        
        print(f"\nğŸ“š ç®—æ³•æ€»ç»“:")
        print("1. K-means - åŸºäºè·ç¦»çš„èšç±»ï¼Œéœ€è¦é¢„è®¾ç°‡æ•°")
        print("2. å±‚æ¬¡èšç±» - æ„å»ºèšç±»æ ‘ï¼Œä¸éœ€è¦é¢„è®¾ç°‡æ•°")
        print("3. PCA - çº¿æ€§é™ç»´ï¼Œä¿æŒæœ€å¤§æ–¹å·®")
        print("4. DBSCAN - å¯†åº¦èšç±»ï¼Œå¯å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡")

def main():
    """ä¸»å‡½æ•°"""
    unsupervised = UnsupervisedLearning()
    unsupervised.run_all_algorithms()
    
    print("\nğŸ’¡ ç®—æ³•é€‰æ‹©æŒ‡å—:")
    print("1. K-means - çƒçŠ¶ç°‡ï¼Œå·²çŸ¥ç°‡æ•°")
    print("2. å±‚æ¬¡èšç±» - æ¢ç´¢æ€§åˆ†æï¼Œæ„å»ºåˆ†ç±»ä½“ç³»")
    print("3. PCA - æ•°æ®å¯è§†åŒ–ï¼Œç‰¹å¾é™ç»´")
    print("4. DBSCAN - ä»»æ„å½¢çŠ¶ç°‡ï¼Œæœ‰å™ªå£°æ•°æ®")

if __name__ == "__main__":
    main()
